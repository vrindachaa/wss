{
  
    
        "post0": {
            "title": "Forest Cover - Data Analysis and Prediction using Machine Learning",
            "content": "Introduction . In this project, we want to use a variety of algorithms and classifiers to predict the type of tree cover in a sample area within Roosevelt National Forest of Northern Colorado given various other features about the area, including the soil type, the vertical and horizontal distance to hydrology, the elevation, etc. We will use the following algorithms to make our predictions: . K-Nearest Neighbor | ExtraTrees | Decision Trees | Random Forest | Light Gradient Boosting Machine | XGBoost | . We will do some data exploration, preprocessing, feature engineering, and other processes to ensure the algorithms we choose can make the best possible predictions. . Exploratory Data Analysis (EDA) . First, let&#39;s go through the exploratory stage of data analysis for this dataset, also called EDA. Here, we seek to understand the relationships between the features by creating graphs of their correlations, distributions, and skewness. Let&#39;s load some libraries we might use in this project. . import numpy as np import pandas as pd pd.set_option(&#39;display.max_columns&#39;, 60) pd.set_option(&#39;display.max_rows&#39;, 60) from IPython.display import display, HTML # for visualization from IPython.core.pylabtools import figsize from matplotlib import pyplot as plt %matplotlib inline # to include graphs inline within the frontends next to code import seaborn as sns sns.set_context(font_scale=2) # to bypass warnings in various dataframe assignments pd.options.mode.chained_assignment = None # machine learning models from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier from sklearn.tree import DecisionTreeClassifier from xgboost import XGBClassifier from lightgbm import LGBMClassifier # preprocessing functions and evaluation models from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.dummy import DummyClassifier from sklearn.preprocessing import StandardScaler . trees = pd.read_csv(&quot;covtype.csv&quot;) print(&quot;The size of the dataset is: &quot;, trees.shape) . The size of the dataset is: (581012, 55) . This whole dataset has 581012 samples and 55 features. Let&#39;s look at the first 15 samples and the last 15 samples to see what some of this data looks like. . display(HTML(trees.head(15).to_html())) . Elevation Aspect Slope Horizontal_Distance_To_Hydrology Vertical_Distance_To_Hydrology Horizontal_Distance_To_Roadways Hillshade_9am Hillshade_Noon Hillshade_3pm Horizontal_Distance_To_Fire_Points Wilderness_Area1 Wilderness_Area2 Wilderness_Area3 Wilderness_Area4 Soil_Type1 Soil_Type2 Soil_Type3 Soil_Type4 Soil_Type5 Soil_Type6 Soil_Type7 Soil_Type8 Soil_Type9 Soil_Type10 Soil_Type11 Soil_Type12 Soil_Type13 Soil_Type14 Soil_Type15 Soil_Type16 Soil_Type17 Soil_Type18 Soil_Type19 Soil_Type20 Soil_Type21 Soil_Type22 Soil_Type23 Soil_Type24 Soil_Type25 Soil_Type26 Soil_Type27 Soil_Type28 Soil_Type29 Soil_Type30 Soil_Type31 Soil_Type32 Soil_Type33 Soil_Type34 Soil_Type35 Soil_Type36 Soil_Type37 Soil_Type38 Soil_Type39 Soil_Type40 Cover_Type . 0 2596 | 51 | 3 | 258 | 0 | 510 | 221 | 232 | 148 | 6279 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 1 2590 | 56 | 2 | 212 | -6 | 390 | 220 | 235 | 151 | 6225 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 2 2804 | 139 | 9 | 268 | 65 | 3180 | 234 | 238 | 135 | 6121 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . 3 2785 | 155 | 18 | 242 | 118 | 3090 | 238 | 238 | 122 | 6211 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . 4 2595 | 45 | 2 | 153 | -1 | 391 | 220 | 234 | 150 | 6172 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 5 2579 | 132 | 6 | 300 | -15 | 67 | 230 | 237 | 140 | 6031 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . 6 2606 | 45 | 7 | 270 | 5 | 633 | 222 | 225 | 138 | 6256 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 7 2605 | 49 | 4 | 234 | 7 | 573 | 222 | 230 | 144 | 6228 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 8 2617 | 45 | 9 | 240 | 56 | 666 | 223 | 221 | 133 | 6244 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 9 2612 | 59 | 10 | 247 | 11 | 636 | 228 | 219 | 124 | 6230 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 10 2612 | 201 | 4 | 180 | 51 | 735 | 218 | 243 | 161 | 6222 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 11 2886 | 151 | 11 | 371 | 26 | 5253 | 234 | 240 | 136 | 4051 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . 12 2742 | 134 | 22 | 150 | 69 | 3215 | 248 | 224 | 92 | 6091 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . 13 2609 | 214 | 7 | 150 | 46 | 771 | 213 | 247 | 170 | 6211 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . 14 2503 | 157 | 4 | 67 | 4 | 674 | 224 | 240 | 151 | 5600 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | . display(HTML(trees.tail(15).to_html())) . Elevation Aspect Slope Horizontal_Distance_To_Hydrology Vertical_Distance_To_Hydrology Horizontal_Distance_To_Roadways Hillshade_9am Hillshade_Noon Hillshade_3pm Horizontal_Distance_To_Fire_Points Wilderness_Area1 Wilderness_Area2 Wilderness_Area3 Wilderness_Area4 Soil_Type1 Soil_Type2 Soil_Type3 Soil_Type4 Soil_Type5 Soil_Type6 Soil_Type7 Soil_Type8 Soil_Type9 Soil_Type10 Soil_Type11 Soil_Type12 Soil_Type13 Soil_Type14 Soil_Type15 Soil_Type16 Soil_Type17 Soil_Type18 Soil_Type19 Soil_Type20 Soil_Type21 Soil_Type22 Soil_Type23 Soil_Type24 Soil_Type25 Soil_Type26 Soil_Type27 Soil_Type28 Soil_Type29 Soil_Type30 Soil_Type31 Soil_Type32 Soil_Type33 Soil_Type34 Soil_Type35 Soil_Type36 Soil_Type37 Soil_Type38 Soil_Type39 Soil_Type40 Cover_Type . 580997 2433 | 168 | 23 | 162 | 41 | 175 | 231 | 241 | 128 | 815 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 580998 2429 | 166 | 24 | 153 | 45 | 162 | 232 | 240 | 125 | 812 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 580999 2426 | 168 | 24 | 150 | 42 | 153 | 231 | 241 | 127 | 811 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581000 2423 | 169 | 24 | 134 | 39 | 150 | 230 | 241 | 128 | 810 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581001 2421 | 172 | 25 | 124 | 35 | 134 | 227 | 242 | 132 | 811 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581002 2419 | 168 | 25 | 108 | 33 | 124 | 230 | 240 | 126 | 812 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581003 2415 | 161 | 25 | 95 | 29 | 120 | 236 | 237 | 116 | 815 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581004 2410 | 158 | 24 | 90 | 24 | 120 | 238 | 236 | 115 | 819 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581005 2405 | 159 | 22 | 90 | 19 | 120 | 237 | 238 | 119 | 824 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581006 2401 | 157 | 21 | 90 | 15 | 120 | 238 | 238 | 119 | 830 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581007 2396 | 153 | 20 | 85 | 17 | 108 | 240 | 237 | 118 | 837 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581008 2391 | 152 | 19 | 67 | 12 | 95 | 240 | 237 | 119 | 845 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581009 2386 | 159 | 17 | 60 | 7 | 90 | 236 | 241 | 130 | 854 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581010 2384 | 170 | 15 | 60 | 5 | 90 | 230 | 245 | 143 | 864 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . 581011 2383 | 165 | 13 | 60 | 4 | 67 | 231 | 244 | 141 | 875 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . Here, we can see that the first 10 columns are continuous variables, describing some distance or angle, while the next 4 columns are one-hot encoded columns for the wilderness area of the sample. The winderness areas correspond with the following areas in the Roosevelt National Forest: . Wilderness area 1 = Rawah Wilderness Area | Wilderness area 2 = Neota Wilderness Area | Wilderness area 3 = Comanche Peak Wilderness Area | Wilderness area 4 = Cache la Poudre Wilderness Area | . After the wilderness area columns, we have again one-hot encoded columns for 40 soil types, depending on what sort of soil the specific sample contains. The last column here is the cover type. This is a discrete variable with values 1-7 depending on the cover type. The numbers correspond with the following tree cover types: . 1- Spruce/Fir | 2- Lodgepole Pine | 3- Ponderosa Pine | 4- Cottonwood/Willow | 5- Aspen | 6- Douglas-fir | 7- Krummholz | . We should also look at the distribution of the values of each column to get a general sense of whether we need to consider any missing values, what the normal range for each feature looks like, and what they describe. The info and describe commands in from the pandas library can be useful for this. . display(pd.DataFrame.info(trees)) print( &#39;Null values? :&#39; , trees.isna().any().any()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 581012 entries, 0 to 581011 Data columns (total 55 columns): # Column Non-Null Count Dtype -- -- 0 Elevation 581012 non-null int64 1 Aspect 581012 non-null int64 2 Slope 581012 non-null int64 3 Horizontal_Distance_To_Hydrology 581012 non-null int64 4 Vertical_Distance_To_Hydrology 581012 non-null int64 5 Horizontal_Distance_To_Roadways 581012 non-null int64 6 Hillshade_9am 581012 non-null int64 7 Hillshade_Noon 581012 non-null int64 8 Hillshade_3pm 581012 non-null int64 9 Horizontal_Distance_To_Fire_Points 581012 non-null int64 10 Wilderness_Area1 581012 non-null int64 11 Wilderness_Area2 581012 non-null int64 12 Wilderness_Area3 581012 non-null int64 13 Wilderness_Area4 581012 non-null int64 14 Soil_Type1 581012 non-null int64 15 Soil_Type2 581012 non-null int64 16 Soil_Type3 581012 non-null int64 17 Soil_Type4 581012 non-null int64 18 Soil_Type5 581012 non-null int64 19 Soil_Type6 581012 non-null int64 20 Soil_Type7 581012 non-null int64 21 Soil_Type8 581012 non-null int64 22 Soil_Type9 581012 non-null int64 23 Soil_Type10 581012 non-null int64 24 Soil_Type11 581012 non-null int64 25 Soil_Type12 581012 non-null int64 26 Soil_Type13 581012 non-null int64 27 Soil_Type14 581012 non-null int64 28 Soil_Type15 581012 non-null int64 29 Soil_Type16 581012 non-null int64 30 Soil_Type17 581012 non-null int64 31 Soil_Type18 581012 non-null int64 32 Soil_Type19 581012 non-null int64 33 Soil_Type20 581012 non-null int64 34 Soil_Type21 581012 non-null int64 35 Soil_Type22 581012 non-null int64 36 Soil_Type23 581012 non-null int64 37 Soil_Type24 581012 non-null int64 38 Soil_Type25 581012 non-null int64 39 Soil_Type26 581012 non-null int64 40 Soil_Type27 581012 non-null int64 41 Soil_Type28 581012 non-null int64 42 Soil_Type29 581012 non-null int64 43 Soil_Type30 581012 non-null int64 44 Soil_Type31 581012 non-null int64 45 Soil_Type32 581012 non-null int64 46 Soil_Type33 581012 non-null int64 47 Soil_Type34 581012 non-null int64 48 Soil_Type35 581012 non-null int64 49 Soil_Type36 581012 non-null int64 50 Soil_Type37 581012 non-null int64 51 Soil_Type38 581012 non-null int64 52 Soil_Type39 581012 non-null int64 53 Soil_Type40 581012 non-null int64 54 Cover_Type 581012 non-null int64 dtypes: int64(55) memory usage: 243.8 MB . None . Null values? : False . Here, we can see all of the columns have non-null values and that the datatype for each is int64. The describe function will tell us more about the numerical distribution of each of these columns (or features). . display(trees.describe()) . Elevation Aspect Slope Horizontal_Distance_To_Hydrology Vertical_Distance_To_Hydrology Horizontal_Distance_To_Roadways Hillshade_9am Hillshade_Noon Hillshade_3pm Horizontal_Distance_To_Fire_Points Wilderness_Area1 Wilderness_Area2 Wilderness_Area3 Wilderness_Area4 Soil_Type1 Soil_Type2 Soil_Type3 Soil_Type4 Soil_Type5 Soil_Type6 Soil_Type7 Soil_Type8 Soil_Type9 Soil_Type10 Soil_Type11 Soil_Type12 Soil_Type13 Soil_Type14 Soil_Type15 Soil_Type16 Soil_Type17 Soil_Type18 Soil_Type19 Soil_Type20 Soil_Type21 Soil_Type22 Soil_Type23 Soil_Type24 Soil_Type25 Soil_Type26 Soil_Type27 Soil_Type28 Soil_Type29 Soil_Type30 Soil_Type31 Soil_Type32 Soil_Type33 Soil_Type34 Soil_Type35 Soil_Type36 Soil_Type37 Soil_Type38 Soil_Type39 Soil_Type40 Cover_Type . count 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | 581012.000000 | . mean 2959.365301 | 155.656807 | 14.103704 | 269.428217 | 46.418855 | 2350.146611 | 212.146049 | 223.318716 | 142.528263 | 1980.291226 | 0.448865 | 0.051434 | 0.436074 | 0.063627 | 0.005217 | 0.012952 | 0.008301 | 0.021335 | 0.002749 | 0.011316 | 0.000181 | 0.000308 | 0.001974 | 0.056168 | 0.021359 | 0.051584 | 0.030001 | 0.001031 | 0.000005 | 0.004897 | 0.005890 | 0.003268 | 0.006921 | 0.015936 | 0.001442 | 0.057439 | 0.099399 | 0.036622 | 0.000816 | 0.004456 | 0.001869 | 0.001628 | 0.198356 | 0.051927 | 0.044175 | 0.090392 | 0.077716 | 0.002773 | 0.003255 | 0.000205 | 0.000513 | 0.026803 | 0.023762 | 0.015060 | 2.051471 | . std 279.984734 | 111.913721 | 7.488242 | 212.549356 | 58.295232 | 1559.254870 | 26.769889 | 19.768697 | 38.274529 | 1324.195210 | 0.497379 | 0.220882 | 0.495897 | 0.244087 | 0.072039 | 0.113066 | 0.090731 | 0.144499 | 0.052356 | 0.105775 | 0.013442 | 0.017550 | 0.044387 | 0.230245 | 0.144579 | 0.221186 | 0.170590 | 0.032092 | 0.002272 | 0.069804 | 0.076518 | 0.057077 | 0.082902 | 0.125228 | 0.037950 | 0.232681 | 0.299197 | 0.187833 | 0.028551 | 0.066605 | 0.043193 | 0.040318 | 0.398762 | 0.221879 | 0.205483 | 0.286743 | 0.267725 | 0.052584 | 0.056957 | 0.014310 | 0.022641 | 0.161508 | 0.152307 | 0.121791 | 1.396504 | . min 1859.000000 | 0.000000 | 0.000000 | 0.000000 | -173.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . 25% 2809.000000 | 58.000000 | 9.000000 | 108.000000 | 7.000000 | 1106.000000 | 198.000000 | 213.000000 | 119.000000 | 1024.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . 50% 2996.000000 | 127.000000 | 13.000000 | 218.000000 | 30.000000 | 1997.000000 | 218.000000 | 226.000000 | 143.000000 | 1710.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | . 75% 3163.000000 | 260.000000 | 18.000000 | 384.000000 | 69.000000 | 3328.000000 | 231.000000 | 237.000000 | 168.000000 | 2550.000000 | 1.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | . max 3858.000000 | 360.000000 | 66.000000 | 1397.000000 | 601.000000 | 7117.000000 | 254.000000 | 254.000000 | 254.000000 | 7173.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 7.000000 | . Outlier Detection &amp; Removal . We can see the ranges and the average value for each function. What we can&#39;t see so far is the distribution, so we don&#39;t know whether any outliers exist and if they create any problems in our analysis. To see this, we&#39;ll define a function to detect outliers in the dataset, using the inter quartile range (IQR) method. In this method, we calculate the first quarentile, Q1 and the third quarentile, Q3, and call the IQR the difference between the two. The IQR will then help us construct a normal range for the dataset using the following equations: $$UL = Q3 + 3 cdot IQR $$ $$LL = Q1 - 3 cdot IQR $$ Here, UL is the upper limit and LL is the lower limit. Values outside of this range will be considered outliers. Typically, 1.5 is used instead of 3 for ourliers, but I personally feel this excludes many samples that might be relevant. 3 is used to determine extreme outliers, so in order to keep our models flexible and accurate, we want to preserve as many rows as possible. This analysis mostly applies to continuous variables, so we will focus on the first 10 columns in selecting which outliers to exclude. We will also not consider the hillshade columns in this, as they have a pre-set range of 0-255, which does not require normalization. . def find_outlier_IQR(df, col_name): &#39;&#39;&#39; This function takes in a dataset and a column name, and creates a normal range based on the IQR, and gives an outlier count. Parameters: a pandas dataframe, a string of column name in the dataframe Output: lower limit int , upper limit int, outlier count int &#39;&#39;&#39; Q1=np.percentile(np.array(df[col_name].tolist()), 25) Q3=np.percentile(np.array(df[col_name].tolist()), 75) IQR=Q3-Q1 UL= Q3 + (3*IQR) LL= Q1 - (3*IQR) outlier_count = 0 for value in df[col_name].tolist(): if (value &lt; LL) | (value &gt; UL): outlier_count +=1 return LL, UL, outlier_count for column in trees[[&#39;Elevation&#39;, &#39;Aspect&#39;, &#39;Slope&#39;, &#39;Horizontal_Distance_To_Hydrology&#39;, &#39;Vertical_Distance_To_Hydrology&#39;, &#39;Horizontal_Distance_To_Roadways&#39;, &#39;Horizontal_Distance_To_Fire_Points&#39;]]: a,b,c =find_outlier_IQR(trees, column) if c &gt; 0: print(&quot;There are {} outliers in {}. The normal range is {} - {}. &quot;.format(c, column, a, b)) . There are 275 outliers in Slope. The normal range is -18.0 - 45.0. There are 414 outliers in Horizontal_Distance_To_Hydrology. The normal range is -720.0 - 1212.0. There are 5339 outliers in Vertical_Distance_To_Hydrology. The normal range is -179.0 - 255.0. There are 10 outliers in Horizontal_Distance_To_Fire_Points. The normal range is -3554.0 - 7128.0. . For this dataset, we will consider outliers from the following columns: . Horizontal_Distance_To_Hydrology | Vertical_Distance_To_Hydrology | Horizontal_Distance_To_Fire_Points | . This is becausem, as discussed earlier, the other columns are discrete variables (ie, one-hot encoded columns like soil type, wilderness area, etc) or have a pre-set range that does not need normalization, like the hillshade columns. We will first remove outliers from the Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology columns, as they seem to have a wide range and a good number of outliers. . trees = trees[(trees[&#39;Horizontal_Distance_To_Hydrology&#39;] &gt; find_outlier_IQR(trees, &#39;Horizontal_Distance_To_Hydrology&#39;)[0]) &amp; (trees[&#39;Horizontal_Distance_To_Hydrology&#39;] &lt; find_outlier_IQR(trees, &#39;Horizontal_Distance_To_Hydrology&#39;)[1])&amp; (trees[&#39;Vertical_Distance_To_Hydrology&#39;] &gt; find_outlier_IQR(trees, &#39;Vertical_Distance_To_Hydrology&#39;)[0]) &amp; (trees[&#39;Vertical_Distance_To_Hydrology&#39;] &lt; find_outlier_IQR(trees, &#39;Vertical_Distance_To_Hydrology&#39;)[1])] print(trees.shape) . (575348, 55) . for column in trees[[&#39;Horizontal_Distance_To_Hydrology&#39;, &#39;Vertical_Distance_To_Hydrology&#39;, &#39;Horizontal_Distance_To_Roadways&#39;, &#39;Horizontal_Distance_To_Fire_Points&#39;]]: a,b,c =find_outlier_IQR(trees, column) if c &gt; 0: print(&quot;There are {} outliers in {}. The normal range is {} - {}. &quot;.format(c, column, a, b)) . There are 66 outliers in Horizontal_Distance_To_Hydrology. The normal range is -705.0 - 1192.0. There are 693 outliers in Vertical_Distance_To_Hydrology. The normal range is -173.0 - 247.0. There are 5 outliers in Horizontal_Distance_To_Fire_Points. The normal range is -3568.0 - 7142.0. . This seems to have trimmed away some of the more extreme outliers from this dataset. Note that the Vertical_Distance_To_Hydrology column still has many outliers because the IQR is recomputing the ranges based on the new data, so these are not true extreme outliers, and we don&#39;t need to worry about them. . Univariate Analysis . Now that we have a bit of description about our dataset, let&#39;s begin univariate analysis on this dataset. In this section, we will look at the skew and densities of the varaibles. First, we have to split up the dataframe to destinguish the different types of variables, like continuous vs discrete (in this case, one-hot encoded) variables. As we saw, the continuous variables are the first 10 variables, so we will group them into a continuous variables list. The wilderness is one category of the ont-hot encoded columns, so will split that into a seperate list, and same with the soil type. This will make visualization easier as we want to avoid comparing different types of variables to one another. . cont_vars = trees.loc[:,&#39;Elevation&#39;:&#39;Horizontal_Distance_To_Fire_Points&#39;] cont_vars_vis = cont_vars.copy() cont_vars_vis[&#39;Cover_Type&#39;] = trees[&#39;Cover_Type&#39;] soiltype = trees.loc[:,&#39;Soil_Type1&#39;:&#39;Soil_Type40&#39;] soiltype_vis = soiltype.copy() soiltype_vis[&#39;Cover_Type&#39;] = trees[&#39;Cover_Type&#39;] wilderness = trees.loc[:,&#39;Wilderness_Area1&#39;:&#39;Wilderness_Area4&#39;] wilderness_vis = wilderness.copy() wilderness_vis[&#39;Cover_Type&#39;] = trees[&#39;Cover_Type&#39;] print(wilderness.shape) print(soiltype.shape) print(cont_vars.shape) . (575348, 4) (575348, 40) (575348, 10) . First, let&#39;s see the distribution of the class, ie the tree cover type. Since we want to see it as a percentage of all cover types, a pie chart can best help us visualize this. . labels= &#39;1- Spruce/Fir&#39;, &#39;2- Lodgepole Pine&#39;, &#39;3- Ponderosa Pine&#39;, &#39;4- Cottonwood/Willow&#39;, &#39;5- Aspen&#39;, &#39;6- Douglas-fir&#39;, &#39;7- Krummholz&#39; trees_dist = trees.groupby(&#39;Cover_Type&#39;).size() fig1, ax1 = plt.subplots() fig1.set_size_inches(15,10) ax1.pie(trees_dist, labels=labels, autopct=&#39;%1.1f%%&#39;) ax1.axis(&#39;equal&#39;) plt.title(&#39;Percentages of Tree Cover Types&#39;,fontsize=20) plt.show() . Clearly, the Lodgepole Pine and the Spruce/Fir are the most common types of trees in this sample. . Let&#39;s take a look at the skew to see if the outlier removal was able to mitigate the skew a bit. Recall skewness for a normal distribution is 0; this means the data is mostly even/symmentrical. Negative skew indicate that the column is &quot;skewed left&quot;, which typically means the left tail, associated with relatively smaller values in the range, is longer than the right tail. Similarly, skewed right means the column has more relatively large values, which are in the right tail, and thus the right tail is longer than the left tail. . skew = trees.skew() display(skew) . Elevation -0.841993 Aspect 0.408319 Slope 0.794381 Horizontal_Distance_To_Hydrology 1.067779 Vertical_Distance_To_Hydrology 1.259936 Horizontal_Distance_To_Roadways 0.711719 Hillshade_9am -1.179253 Hillshade_Noon -1.069709 Hillshade_3pm -0.286396 Horizontal_Distance_To_Fire_Points 1.284157 Wilderness_Area1 0.194811 Wilderness_Area2 4.039776 Wilderness_Area3 0.272381 Wilderness_Area4 3.561498 Soil_Type1 13.673068 Soil_Type2 8.579879 Soil_Type3 10.785470 Soil_Type4 6.610576 Soil_Type5 18.901657 Soil_Type6 9.207115 Soil_Type7 74.003605 Soil_Type8 56.667907 Soil_Type9 22.329706 Soil_Type10 3.843037 Soil_Type11 6.594597 Soil_Type12 4.032469 Soil_Type13 5.573565 Soil_Type14 30.943824 Soil_Type15 437.927696 Soil_Type16 14.115144 Soil_Type17 12.850639 Soil_Type18 17.319904 Soil_Type19 11.836113 Soil_Type20 7.691293 Soil_Type21 26.145321 Soil_Type22 3.781742 Soil_Type23 2.660565 Soil_Type24 4.934947 Soil_Type25 34.796860 Soil_Type26 14.806537 Soil_Type27 26.897112 Soil_Type28 27.351641 Soil_Type29 1.501153 Soil_Type30 4.026299 Soil_Type31 4.454534 Soil_Type32 2.860704 Soil_Type33 3.195296 Soil_Type34 18.985754 Soil_Type35 17.356867 Soil_Type36 69.511744 Soil_Type37 43.905678 Soil_Type38 5.836805 Soil_Type39 6.242743 Soil_Type40 8.560924 Cover_Type 2.278729 dtype: float64 . It seems the skewness is somewhat close to 0 for the continuous variables; let&#39;s visualize this to see what it looks like. This will help us get a better understanding of the skewnesss of variables relative to one another. . skew =pd.DataFrame(skew,index=None,columns=[&#39;Skewness&#39;]) plt.figure(figsize=(15,7)) sns.barplot(x=skew.index,y=skew.Skewness) plt.xticks(rotation=90) . (array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]), [Text(0, 0, &#39;Elevation&#39;), Text(1, 0, &#39;Aspect&#39;), Text(2, 0, &#39;Slope&#39;), Text(3, 0, &#39;Horizontal_Distance_To_Hydrology&#39;), Text(4, 0, &#39;Vertical_Distance_To_Hydrology&#39;), Text(5, 0, &#39;Horizontal_Distance_To_Roadways&#39;), Text(6, 0, &#39;Hillshade_9am&#39;), Text(7, 0, &#39;Hillshade_Noon&#39;), Text(8, 0, &#39;Hillshade_3pm&#39;), Text(9, 0, &#39;Horizontal_Distance_To_Fire_Points&#39;), Text(10, 0, &#39;Wilderness_Area1&#39;), Text(11, 0, &#39;Wilderness_Area2&#39;), Text(12, 0, &#39;Wilderness_Area3&#39;), Text(13, 0, &#39;Wilderness_Area4&#39;), Text(14, 0, &#39;Soil_Type1&#39;), Text(15, 0, &#39;Soil_Type2&#39;), Text(16, 0, &#39;Soil_Type3&#39;), Text(17, 0, &#39;Soil_Type4&#39;), Text(18, 0, &#39;Soil_Type5&#39;), Text(19, 0, &#39;Soil_Type6&#39;), Text(20, 0, &#39;Soil_Type7&#39;), Text(21, 0, &#39;Soil_Type8&#39;), Text(22, 0, &#39;Soil_Type9&#39;), Text(23, 0, &#39;Soil_Type10&#39;), Text(24, 0, &#39;Soil_Type11&#39;), Text(25, 0, &#39;Soil_Type12&#39;), Text(26, 0, &#39;Soil_Type13&#39;), Text(27, 0, &#39;Soil_Type14&#39;), Text(28, 0, &#39;Soil_Type15&#39;), Text(29, 0, &#39;Soil_Type16&#39;), Text(30, 0, &#39;Soil_Type17&#39;), Text(31, 0, &#39;Soil_Type18&#39;), Text(32, 0, &#39;Soil_Type19&#39;), Text(33, 0, &#39;Soil_Type20&#39;), Text(34, 0, &#39;Soil_Type21&#39;), Text(35, 0, &#39;Soil_Type22&#39;), Text(36, 0, &#39;Soil_Type23&#39;), Text(37, 0, &#39;Soil_Type24&#39;), Text(38, 0, &#39;Soil_Type25&#39;), Text(39, 0, &#39;Soil_Type26&#39;), Text(40, 0, &#39;Soil_Type27&#39;), Text(41, 0, &#39;Soil_Type28&#39;), Text(42, 0, &#39;Soil_Type29&#39;), Text(43, 0, &#39;Soil_Type30&#39;), Text(44, 0, &#39;Soil_Type31&#39;), Text(45, 0, &#39;Soil_Type32&#39;), Text(46, 0, &#39;Soil_Type33&#39;), Text(47, 0, &#39;Soil_Type34&#39;), Text(48, 0, &#39;Soil_Type35&#39;), Text(49, 0, &#39;Soil_Type36&#39;), Text(50, 0, &#39;Soil_Type37&#39;), Text(51, 0, &#39;Soil_Type38&#39;), Text(52, 0, &#39;Soil_Type39&#39;), Text(53, 0, &#39;Soil_Type40&#39;), Text(54, 0, &#39;Cover_Type&#39;)]) . plt.figure(figsize=(14, 28)) for i,col in enumerate(cont_vars.columns.values): l,u,ct = find_outlier_IQR(cont_vars_vis,col) plt.subplot(5,2,i+1) sns.kdeplot(cont_vars[col]) plt.axvline(x=l, color = &#39;g&#39;) plt.axvline(x=u, color = &#39;r&#39;) plt.show() . It seems our observations were correct, the most skewed features are the discrete or one-hot encoded features. This is probably not an issue since there is also a bias in the class representation (two types of trees are over-represented). We can see if we can explore some relationships between features during the bivariate and multivariate analysis. . Bivariate Analysis . Let&#39;s now begin the bivariate analysis phase. In bivariate analysis, we are looking at the relationship between features. First, we can look at the below boxplots to see each continuous variable&#39;s frequency with respect to the cover type. . for i, col in enumerate(cont_vars.columns): plt.figure(i,figsize=(10,4)) ax = sns.boxplot(x=cont_vars_vis[&#39;Cover_Type&#39;], y=col, data=cont_vars_vis, palette=&quot;rocket&quot;) ax.set_xticklabels(labels, rotation=30) plt.show() . We can now take a look at the correlation of the variables to the cover type. First, lets see a summary; then, we can visualize these. . corr = trees.corr()[&#39;Cover_Type&#39;] corr = pd.DataFrame(data =corr) corr = corr.rename({&#39;Cover_Type&#39;: &#39;Correlation&#39;}, axis = 1) print(corr) . Correlation Elevation -0.282160 Aspect 0.015804 Slope 0.152433 Horizontal_Distance_To_Hydrology -0.032768 Vertical_Distance_To_Hydrology 0.079644 Horizontal_Distance_To_Roadways -0.161764 Hillshade_9am -0.034286 Hillshade_Noon -0.098474 Hillshade_3pm -0.051531 Horizontal_Distance_To_Fire_Points -0.113677 Wilderness_Area1 -0.202592 Wilderness_Area2 -0.047817 Wilderness_Area3 0.063319 Wilderness_Area4 0.327001 Soil_Type1 0.091900 Soil_Type2 0.119445 Soil_Type3 0.068978 Soil_Type4 0.099597 Soil_Type5 0.078785 Soil_Type6 0.114294 Soil_Type7 -0.000464 Soil_Type8 -0.003655 Soil_Type9 -0.006050 Soil_Type10 0.247147 Soil_Type11 0.035921 Soil_Type12 -0.023214 Soil_Type13 0.025601 Soil_Type14 0.066283 Soil_Type15 0.006493 Soil_Type16 0.010127 Soil_Type17 0.091667 Soil_Type18 0.007615 Soil_Type19 -0.036583 Soil_Type20 -0.028604 Soil_Type21 -0.025545 Soil_Type22 -0.142501 Soil_Type23 -0.135571 Soil_Type24 -0.068260 Soil_Type25 -0.006435 Soil_Type26 -0.000200 Soil_Type27 -0.020906 Soil_Type28 -0.001495 Soil_Type29 -0.124597 Soil_Type30 -0.009967 Soil_Type31 -0.064387 Soil_Type32 -0.075585 Soil_Type33 -0.060780 Soil_Type34 0.004903 Soil_Type35 0.081246 Soil_Type36 0.025681 Soil_Type37 0.081109 Soil_Type38 0.160906 Soil_Type39 0.156542 Soil_Type40 0.112593 Cover_Type 1.000000 . ax = cont_vars_vis.corr()[&quot;Cover_Type&quot;].plot(kind=&quot;bar&quot;) ax.axhline(linewidth=0.5,y=0, color = &#39;k&#39;) . &lt;matplotlib.lines.Line2D at 0x17e00d69df0&gt; . ax= soiltype_vis.corr()[&quot;Cover_Type&quot;].plot(kind=&quot;bar&quot;) ax.axhline(linewidth=0.5,y=0, color = &#39;k&#39;) . &lt;matplotlib.lines.Line2D at 0x17e6014c7f0&gt; . ax = wilderness_vis.corr()[&quot;Cover_Type&quot;].plot(kind=&quot;bar&quot;) ax.axhline(linewidth=0.5,y=0, color = &#39;k&#39;) . &lt;matplotlib.lines.Line2D at 0x17e6016dd00&gt; . This tells us a lot about the correlation with respect to the class. We can also explore the correlation between every pair of features with the heat map of a correlation matrix below. . plt.figure(figsize=(50,50)) sns.heatmap(trees.corr(),cmap=&#39;BuGn&#39;,linecolor=&#39;white&#39;,linewidths=1,annot=True, xticklabels = True, yticklabels= True) . &lt;AxesSubplot:&gt; . plt.figure(figsize=(8,8)) sns.heatmap(cont_vars_vis.corr(),cmap=&#39;BuGn&#39;,linecolor=&#39;white&#39;,linewidths=1,annot=True, xticklabels = True, yticklabels= True) . &lt;AxesSubplot:&gt; . plt.figure(figsize=(40,40)) disc = trees.drop(trees.columns[0:10], axis=1) sns.heatmap(disc.corr(),cmap=&#39;BuGn&#39;,linecolor=&#39;white&#39;,linewidths=1,annot=True, xticklabels = True, yticklabels= True) . &lt;AxesSubplot:&gt; . Let&#39;s visualize what some of these relationships between variables look like, at least with the continuous data. . plt.figure(figsize=(8,8)) x = sns.PairGrid(cont_vars) x.map(plt.scatter) . &lt;seaborn.axisgrid.PairGrid at 0x17e2c723220&gt; . &lt;Figure size 576x576 with 0 Axes&gt; . Conclusions from EDA . Our left-skewed variables are: . Elevation | Hillshade_9am | Hillshade_Noon | Hillshade_3pm | Horizontal_Distance_To_Fire_Points | . Our right-skewed variables are: . Slope | Horizontal_Distance_To_Roadways | . The correlation offers some insight into the trends between variables. From the heat map, we can see the following positive and negative relationships: . Positive correlation pairs: . Hillshade_9am and Aspect | Hillshade_3pm and Aspect | Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology | Horizontal_Distance_To_Roadways and Horizontal_Distance_To_Fire_Points | Hillshade_3pm and Hillshade_Noon | . Negative correlation pairs: . Elevation and Horizontal_Distance_To_Hydrology | Elevation and Horizontal_Distance_To_Roadways | Aspect and Hillshade_9am | Slope and Hillshade_Noon | Hillshade_9am and Hillshade_3pm | Wilderness_Area3 and Wilderness_Area1 | . Predicting trees with tree-based algorithms . We are finally ready to do some analysis on this dataset. Although we haven&#39;t done any sort of feature engineering to remove or add features yet, we will use tree-based algorithms to figure out some important features in the dataset. One helpful aspect of tree-based algorithms is that they do not require type c data preprocessing-- that is, normalization, feature selection/ dimensionality reduction, log transformation, etc. Thus, we can use this algorithm to simultaneously get started on our data analysis and get some insight into important features. . Establishing a Control Group . First, we will create a dummy algorithm to get a sort of control group for our later algorithms. This is also called a common-sense baseline, and it serves as a metric to make sure our algorithm is actually improving on an algorithm that follows minimal rules, as the DummyClassifier in scikit-learn does. . acc =[] algs = [] trees_training = trees.drop(&#39;Cover_Type&#39;, axis=1) labels_training = trees[&quot;Cover_Type&quot;] train, test, train_labels, test_labels = train_test_split(trees_training, labels_training, test_size=0.3, random_state=1) dummy = DummyClassifier(strategy=&#39;stratified&#39;, random_state=1) # training the model with the training data (70% of our dataset) dummy.fit(train, train_labels) # get accuracy score, add it to our list for comparison later baseline_accuracy = dummy.score(test, test_labels) acc.append(baseline_accuracy) algs.append(&#39;Dummy Classifier&#39;) print(&quot;Our dummy algorithm classified {:0.2f}% of the of the trees correctly&quot;.format(baseline_accuracy*100)) . Our dummy algorithm classified 37.65% of the of the trees correctly . So now we know we have to improve upon a 38% accuracy rate! I&#39;m sure the decision trees and random forest algorithms can help with that. . Decision Tree . Since decision trees are one of the more basic tree-based algorithms, I wanted to see how it compared in this setting to random forests and ExtraTrees. Let&#39;s take a look at what the implementation of this looks like. . dtree = DecisionTreeClassifier() dtree.fit(train, train_labels) dectree_pred = dtree.predict(test) dtree_accuracy = accuracy_score(dectree_pred , test_labels) acc.append(dtree_accuracy) feat_dt = dtree.feature_importances_ algs.append(&#39;Decision Tree&#39;) print(&quot;Our decision tree classified {:0.2f}% of the of the trees correctly&quot;.format(dtree_accuracy*100)) . Our decision tree classified 93.55% of the of the trees correctly . Looks like the decision tree was pretty accurate! Let&#39;s take a look at what features this algorithm found important in prediction. . plt.figure(figsize=(10,15)) plt.barh(train.columns.values, feat_dt) plt.title(&#39;Feature Importance for Decision Tree Algorithm&#39;,fontsize=10) plt.ylabel(&#39;Feature Name&#39;) plt.xlabel(&#39;Relevance&#39;) plt.show() . Random Forest . rf = RandomForestClassifier() rf.fit(train, train_labels) rf_pred = rf.predict(test) rf_acc = accuracy_score(rf_pred , test_labels) acc.append(rf_acc) algs.append(&#39;Random Forest&#39;) feat_rf = rf.feature_importances_ print(&quot;Our random forest classified {:0.2f}% of the of the trees correctly&quot;.format(rf_acc*100)) . Our random forest classified 95.22% of the of the trees correctly . So we were able to obtain a 2% increase in accuracy! That&#39;s certainly quite an improvement, and although it took a lot longer than the decision tree. This is generally why many prefer random forests for accuracy, and why random forests are so popular in classification problems. Let&#39;s see if the feature importance is the same here. . plt.figure(figsize=(10,15)) plt.barh(train.columns.values, feat_rf) plt.title(&#39;Feature Importance for Random Forest Algorithm&#39;,fontsize=10) plt.ylabel(&#39;Feature Name&#39;) plt.xlabel(&#39;Relevance&#39;) plt.show() . ExtraTrees . etree = ExtraTreesClassifier() etree.fit(train, train_labels) etree_pred = etree.predict(test) etree_acc = accuracy_score(etree_pred , test_labels) acc.append(etree_acc) algs.append(&#39;ExtraTrees&#39;) feat_et = etree.feature_importances_ print(&quot;ExtraTrees classified {:0.2f}% of the of the trees correctly&quot;.format(etree_acc*100)) . ExtraTrees classified 95.08% of the of the trees correctly . This algorithm actually performed somewhat on par with the random forest, although the random forest still beat it by a small margin. . plt.figure(figsize=(10,15)) plt.barh(train.columns.values,feat_et) plt.title(&#39;Feature Importance for ExtraTrees Algorithm&#39;,fontsize=10) plt.ylabel(&#39;Feature Name&#39;) plt.xlabel(&#39;Relevance&#39;) plt.show() . Feature Selection . I have a feeling some soil types are under-represented. To investigate this, I will create a method that will turn soil type into a numerical value, or reverse-encode it. Then, we can create a pie chart to see which soil types are most common. . def reverse_encode(relevant_subset): &#39;&#39;&#39; This function reverses one-hot encoding for into rank-encoded representations Parameters: an subset of relevant columns of the same type, which we are trying to reverse encode Output: a title-less list of the encoded variable in numerical data &#39;&#39;&#39; num_list =[] for i in relevant_subset.iloc: ix = 1 for j in i: if j ==1: num_list.append(ix) else: ix +=1 print(&#39;The new list has the following elements: &#39; + str(set(num_list))) if relevant_subset.shape[0] == len(num_list): return num_list else: print(&#39;Error in compiling list: lengths of the two lists do not match.&#39;) w = reverse_encode(wilderness) print(len(w)) . The new list has the following elements: {1, 2, 3, 4} 575348 . s = reverse_encode(soiltype) print(len(s)) . The new list has the following elements: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40} 575348 . We were able to unencode the one-hot encoded variables successfully. Now let&#39;s see the plot chart for the wilderness area. Since soil type has more than 7 classes, its not as effective to visualize it with a pie chart. We can use a bar chart instead. . lab =list(range(1,5)) w = pd.DataFrame(w, columns=[&#39;Wilderness&#39;]) wild_dist = w.groupby(&#39;Wilderness&#39;).size() fig1, ax1 = plt.subplots() fig1.set_size_inches(15,10) ax1.pie(wild_dist, labels= lab, autopct=&#39;%1.1f%%&#39;) ax1.axis(&#39;equal&#39;) plt.title(&#39;Percentages of Wilderness Areas&#39;,fontsize=20) plt.show() . s = pd.DataFrame(s,columns= [&#39;Soil&#39;] ) soil_dist = s.groupby(&#39;Soil&#39;).size() print(&#39;Soil type with percentage of total&#39;) print((soil_dist/sum(soil_dist))*100) . Soil type with percentage of total Soil 1 0.526464 2 1.305471 3 0.838101 4 2.142356 5 0.277571 6 1.139484 7 0.018250 8 0.031112 9 0.199358 10 5.646843 11 2.152089 12 5.206762 13 2.938222 14 0.104111 15 0.000521 16 0.494483 17 0.594770 18 0.330061 19 0.698881 20 1.609287 21 0.145651 22 5.800489 23 10.033058 24 3.660915 25 0.082385 26 0.449989 27 0.137656 28 0.133137 29 19.985296 30 5.220319 31 4.386563 32 9.021670 33 7.617651 34 0.275138 35 0.328671 36 0.020683 37 0.051795 38 2.699757 39 2.383948 40 1.311033 dtype: float64 . print(&#39;The following columns have the lowest correlation with the cover types: n&#39; + str(corr.abs().sort_values(by = &#39;Correlation&#39;).head(10))) . The following columns have the lowest correlation with the cover types: Correlation Soil_Type26 0.000200 Soil_Type7 0.000464 Soil_Type28 0.001495 Soil_Type8 0.003655 Soil_Type34 0.004903 Soil_Type9 0.006050 Soil_Type25 0.006435 Soil_Type15 0.006493 Soil_Type18 0.007615 Soil_Type30 0.009967 . trees_num = trees.loc[:,&#39;Elevation&#39;:&#39;Horizontal_Distance_To_Fire_Points&#39;] trees_num[&#39;Wilderness&#39;] = w.values trees_num[&#39;Soil&#39;] = s.values trees_num[&#39;Cover&#39;] = trees[&quot;Cover_Type&quot;] corr = trees_num.corr()[&#39;Cover&#39;] corr = pd.DataFrame(data =corr) corr = corr.rename({&#39;Cover&#39;: &#39;Correlation&#39;}, axis = 1) print( &#39;Null values? :&#39; , trees_num.isna().any().any()) print(corr) . Null values? : False Correlation Elevation -0.282160 Aspect 0.015804 Slope 0.152433 Horizontal_Distance_To_Hydrology -0.032768 Vertical_Distance_To_Hydrology 0.079644 Horizontal_Distance_To_Roadways -0.161764 Hillshade_9am -0.034286 Hillshade_Noon -0.098474 Hillshade_3pm -0.051531 Horizontal_Distance_To_Fire_Points -0.113677 Wilderness 0.275180 Soil -0.171057 Cover 1.000000 . plt.figure(figsize=(10,10)) sns.heatmap(trees_num.corr(),cmap=&#39;BuGn&#39;,linecolor=&#39;white&#39;,linewidths=1,annot=True, xticklabels = True, yticklabels= True) . &lt;AxesSubplot:&gt; . Thus, these columns will be least helpful in predicting the cover type, which is the goal of this project. Let&#39;s see if we can find out more about which features we should remove. First, note . Tree-based algorithms and feature selection . To make the next couple algorithms as efficient as possible, we can use some of the information we gathered from thetree-based classifiers to see which features are the least important in prediction. The following tables show us the features with the lowest importance values in the random forest classifier, which performed the best in our tree-based analysis. Removing those features can help us reduce noise in the later algorithms&#39; predictions. . feat_df = pd.DataFrame(zip(feat_rf,feat_et, feat_rf), index=[train.columns], columns= [&#39;RF&#39;, &#39;ET&#39;, &#39;DT&#39;]) print(feat_df.shape) print(&#39;The following features have the least importance in the random forest classifier: n&#39; + str(feat_df.sort_values(by = &#39;RF&#39;).head(15))) l = [np.mean(feat_rf), np.mean(feat_dt), np.mean(feat_et)] print(&#39;Mean:&#39; + str(np.mean(l))) . (54, 3) The following features have the least importance in the random forest classifier: RF ET DT Soil_Type15 0.000005 0.000007 0.000005 Soil_Type7 0.000010 0.000046 0.000010 Soil_Type8 0.000043 0.000052 0.000043 Soil_Type36 0.000082 0.000142 0.000082 Soil_Type9 0.000116 0.000240 0.000116 Soil_Type28 0.000165 0.000259 0.000165 Soil_Type25 0.000186 0.000371 0.000186 Soil_Type18 0.000203 0.000591 0.000203 Soil_Type14 0.000386 0.000648 0.000386 Soil_Type26 0.000419 0.000936 0.000419 Soil_Type5 0.000479 0.000724 0.000479 Soil_Type27 0.000588 0.000743 0.000588 Soil_Type34 0.000609 0.000944 0.000609 Soil_Type37 0.000629 0.000759 0.000629 Soil_Type21 0.000685 0.001148 0.000685 Mean:0.018518518518518517 . The above chart displays the features previous tree-based algorithms determined were the least important. I&#39;m interested to see how important soil type and wilderness area is overall. Let&#39;s run a new random forest without the one-hot encoding for soil type and wilderness. . trees_tr = trees_num.drop(&#39;Cover&#39;, axis = 1) labels_tr = trees_num[&#39;Cover&#39;] train, test, train_labels, test_labels = train_test_split(trees_tr, labels_training, test_size=0.3, random_state=1) rf = RandomForestClassifier() rf.fit(train, train_labels) rf_pred = rf.predict(test) rf_acc = accuracy_score(rf_pred , test_labels) feat_rf = rf.feature_importances_ print(&quot;Our random forest classified {:0.2f}% of the of the trees correctly&quot;.format(rf_acc*100)) . Our random forest classified 96.11% of the of the trees correctly . It seems the one-hot encoding did create a bit of noise, because this random forest was able to classify the tree cover type much better. Let&#39;s look at the feature relevance. . plt.figure(figsize=(10,15)) plt.barh(train.columns.values,feat_rf) plt.title(&#39;Feature Importance for Random Forest Algorithm&#39;,fontsize=10) plt.ylabel(&#39;Feature Name&#39;) plt.xlabel(&#39;Relevance&#39;) plt.show() . I&#39;m going to drop the column &#39;Hillshade_3pm&#39; since it is strongly corelated with the &#39;Hillshade_Noon&#39; column as well as the Aspect column, but also has low relevance to the model. In addition, I will be adding some other columns related to the continuous variables: . Euclidean distance to hydrology | Linear combination of Elevation and Aspect | Linear combination of Horizontal_Distance_To_Roadways and Horizontal_Distance_To_Fire_Points | Linear combination of Horizontal_Distance_To_Roadways and Horizontal_Distance_To_Hydrology | Linear combination of Horizontal_Distance_To_Roadways and Vertical_Distance_To_Hydrology | . In addition, I&#39;m going to normalize the Hillshade columns (in this case, divide them by 254), and perform a log transform on the Elevation and Horizontal Distance columns, as they have positive values. We will also perform square root on the Elevation and Horizontal Distance columns. This will help normalize some of those large values, and we will see the correlation and the random forest classifier feature importance to determine which features we will keep. . df = trees_num.copy() df[&#39;Euc_Distance_To_Hydrology&#39;] = (trees_num[&#39;Horizontal_Distance_To_Hydrology&#39;]**2 + trees_num[&#39;Vertical_Distance_To_Hydrology&#39;]**2)**0.5 df[&#39;LC_Horizontal_Roadways_FirePoints&#39;] = 0.5*(trees_num[&#39;Horizontal_Distance_To_Roadways&#39;] + trees_num[&#39;Horizontal_Distance_To_Fire_Points&#39;]) df[&#39;LC_Horizontal_Roadways_Hydrology&#39;] = 0.5*(trees_num[&#39;Horizontal_Distance_To_Roadways&#39;] + trees_num[&#39;Horizontal_Distance_To_Hydrology&#39;]) df[&#39;LC_Horizontal_Roadways_Vertical_Hydrology&#39;] = 0.5*(trees_num[&#39;Horizontal_Distance_To_Roadways&#39;] + trees_num[&#39;Vertical_Distance_To_Hydrology&#39;]) # Sqrt columns df[&#39;sqrt_Elevation&#39;] =(trees_num[&#39;Elevation&#39;])**0.5 df[&#39;sqrt_Horizontal_Distance_To_Hydrology&#39;] =(trees_num[&#39;Horizontal_Distance_To_Hydrology&#39;])**0.5 df[&#39;sqrt_Horizontal_Distance_To_Roadways&#39;] =(trees_num[&#39;Horizontal_Distance_To_Roadways&#39;])**0.5 df[&#39;sqrt_Horizontal_Distance_To_Fire_Points&#39;] =(trees_num[&#39;Horizontal_Distance_To_Fire_Points&#39;])**0.5 df[&#39;sqrt_Euc_Distance_To_Hydrology&#39;] = (df[&#39;Euc_Distance_To_Hydrology&#39;])**0.5 # Normalize Hillshade &amp; drop the Hillshade_3pm df = df.drop([&#39;Hillshade_3pm&#39;], axis = 1) df[&#39;norm_Hillshade_Noon&#39;] = df[&#39;Hillshade_Noon&#39;]/254 df[&#39;norm_Hillshade_9am&#39;] = df[&#39;Hillshade_9am&#39;]/254 df[&#39;norm_Aspect&#39;] = df[&#39;Aspect&#39;]/360 #print(df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]) print( &#39;Null values? :&#39; , df.isna().any().any()) corr = df.corr()[&#39;Cover&#39;] corr = pd.DataFrame(data =corr) corr = corr.rename({&#39;Cover&#39;: &#39;Correlation&#39;}, axis = 1) corr_vis = corr.abs() print(corr_vis.sort_values(by = &#39;Correlation&#39;)) . Null values? : False Correlation norm_Aspect 0.015804 Aspect 0.015804 Euc_Distance_To_Hydrology 0.026680 sqrt_Euc_Distance_To_Hydrology 0.032600 Horizontal_Distance_To_Hydrology 0.032768 Hillshade_9am 0.034286 norm_Hillshade_9am 0.034286 sqrt_Horizontal_Distance_To_Hydrology 0.038833 Vertical_Distance_To_Hydrology 0.079644 norm_Hillshade_Noon 0.098474 Hillshade_Noon 0.098474 Horizontal_Distance_To_Fire_Points 0.113677 sqrt_Horizontal_Distance_To_Fire_Points 0.123971 Slope 0.152433 LC_Horizontal_Roadways_Vertical_Hydrology 0.159253 Horizontal_Distance_To_Roadways 0.161764 LC_Horizontal_Roadways_Hydrology 0.163136 sqrt_Horizontal_Distance_To_Roadways 0.169593 LC_Horizontal_Roadways_FirePoints 0.170957 Soil 0.171057 Wilderness 0.275180 Elevation 0.282160 sqrt_Elevation 0.292478 Cover 1.000000 . print(&#39;Nan values?: &#39;+str(np.any(np.isnan(df)))) print(&#39;No inf values?: &#39;+str(np.all(np.isfinite(df)))) display(pd.DataFrame.describe(df)) display(pd.DataFrame.info(df)) . Nan values?: False No inf values?: True . Elevation Aspect Slope Horizontal_Distance_To_Hydrology Vertical_Distance_To_Hydrology Horizontal_Distance_To_Roadways Hillshade_9am Hillshade_Noon Horizontal_Distance_To_Fire_Points Wilderness Soil Cover Euc_Distance_To_Hydrology LC_Horizontal_Roadways_FirePoints LC_Horizontal_Roadways_Hydrology LC_Horizontal_Roadways_Vertical_Hydrology sqrt_Elevation sqrt_Horizontal_Distance_To_Hydrology sqrt_Horizontal_Distance_To_Roadways sqrt_Horizontal_Distance_To_Fire_Points sqrt_Euc_Distance_To_Hydrology norm_Hillshade_Noon norm_Hillshade_9am norm_Aspect . count 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | 575348.000000 | . mean 2957.298755 | 155.361991 | 14.053832 | 264.080025 | 43.877116 | 2351.681219 | 212.312809 | 223.345940 | 1982.959816 | 2.109077 | 24.305933 | 2.047775 | 270.189276 | 2167.320517 | 1307.880622 | 1197.779168 | 54.317359 | 14.810523 | 45.595469 | 42.144371 | 14.989725 | 0.879315 | 0.835877 | 0.431561 | . std 279.109287 | 111.821993 | 7.469048 | 205.310986 | 52.303259 | 1561.492147 | 26.582840 | 19.757596 | 1328.313906 | 1.062219 | 9.473156 | 1.389914 | 208.685038 | 1180.384174 | 794.796065 | 779.974294 | 2.631204 | 6.687938 | 16.514688 | 14.380965 | 6.745184 | 0.077786 | 0.104657 | 0.310617 | . min 1859.000000 | 0.000000 | 0.000000 | 0.000000 | -173.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 21.000000 | 0.000000 | -45.500000 | 43.116122 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 2807.000000 | 58.000000 | 9.000000 | 108.000000 | 7.000000 | 1106.000000 | 199.000000 | 213.000000 | 1022.000000 | 1.000000 | 20.000000 | 1.000000 | 108.226614 | 1273.500000 | 679.000000 | 577.000000 | 52.981129 | 10.392305 | 33.256578 | 31.968735 | 10.403202 | 0.838583 | 0.783465 | 0.161111 | . 50% 2995.000000 | 127.000000 | 13.000000 | 216.000000 | 29.000000 | 1998.000000 | 218.000000 | 226.000000 | 1712.000000 | 2.000000 | 29.000000 | 2.000000 | 226.901960 | 1954.500000 | 1127.000000 | 1020.000000 | 54.726593 | 14.696938 | 44.698993 | 41.376322 | 15.063265 | 0.889764 | 0.858268 | 0.352778 | . 75% 3162.000000 | 260.000000 | 18.000000 | 379.000000 | 67.000000 | 3331.000000 | 231.000000 | 237.000000 | 2552.000000 | 3.000000 | 31.000000 | 2.000000 | 389.630594 | 2820.500000 | 1814.000000 | 1687.000000 | 56.231664 | 19.467922 | 57.714816 | 50.517324 | 19.739063 | 0.933071 | 0.909449 | 0.722222 | . max 3858.000000 | 360.000000 | 66.000000 | 1211.000000 | 254.000000 | 7117.000000 | 254.000000 | 254.000000 | 7173.000000 | 4.000000 | 40.000000 | 7.000000 | 1235.336796 | 6252.000000 | 3655.500000 | 3562.000000 | 62.112801 | 34.799425 | 84.362314 | 84.693565 | 35.147358 | 1.000000 | 1.000000 | 1.000000 | . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 575348 entries, 0 to 581011 Data columns (total 24 columns): # Column Non-Null Count Dtype -- -- 0 Elevation 575348 non-null int64 1 Aspect 575348 non-null int64 2 Slope 575348 non-null int64 3 Horizontal_Distance_To_Hydrology 575348 non-null int64 4 Vertical_Distance_To_Hydrology 575348 non-null int64 5 Horizontal_Distance_To_Roadways 575348 non-null int64 6 Hillshade_9am 575348 non-null int64 7 Hillshade_Noon 575348 non-null int64 8 Horizontal_Distance_To_Fire_Points 575348 non-null int64 9 Wilderness 575348 non-null int64 10 Soil 575348 non-null int64 11 Cover 575348 non-null int64 12 Euc_Distance_To_Hydrology 575348 non-null float64 13 LC_Horizontal_Roadways_FirePoints 575348 non-null float64 14 LC_Horizontal_Roadways_Hydrology 575348 non-null float64 15 LC_Horizontal_Roadways_Vertical_Hydrology 575348 non-null float64 16 sqrt_Elevation 575348 non-null float64 17 sqrt_Horizontal_Distance_To_Hydrology 575348 non-null float64 18 sqrt_Horizontal_Distance_To_Roadways 575348 non-null float64 19 sqrt_Horizontal_Distance_To_Fire_Points 575348 non-null float64 20 sqrt_Euc_Distance_To_Hydrology 575348 non-null float64 21 norm_Hillshade_Noon 575348 non-null float64 22 norm_Hillshade_9am 575348 non-null float64 23 norm_Aspect 575348 non-null float64 dtypes: float64(12), int64(12) memory usage: 125.9 MB . None . trees_tr = df.drop(&#39;Cover&#39;, axis = 1) labels_tr = df[&#39;Cover&#39;] train, test, train_labels, test_labels = train_test_split(trees_tr, labels_training, test_size=0.3, random_state=1) rf = RandomForestClassifier() rf.fit(train, train_labels) rf_pred = rf.predict(test) rf_acc = accuracy_score(rf_pred , test_labels) feat_rf_new = rf.feature_importances_ print(&quot;Our random forest classified {:0.2f}% of the of the trees correctly&quot;.format(rf_acc*100)) . Our random forest classified 96.94% of the of the trees correctly . l = pd.DataFrame(zip(train.columns.values,feat_rf_new)) print(l.sort_values(by = 1)) plt.figure(figsize=(10,15)) plt.barh(train.columns.values,feat_rf_new) plt.title(&#39;Feature Importance for Random Forest Algorithm&#39;,fontsize=10) plt.ylabel(&#39;Feature Name&#39;) plt.xlabel(&#39;Relevance&#39;) plt.show() . 0 1 2 Slope 0.016840 3 Horizontal_Distance_To_Hydrology 0.017300 16 sqrt_Horizontal_Distance_To_Hydrology 0.018300 7 Hillshade_Noon 0.020102 20 norm_Hillshade_Noon 0.020357 6 Hillshade_9am 0.020595 21 norm_Hillshade_9am 0.020917 1 Aspect 0.021531 22 norm_Aspect 0.021572 11 Euc_Distance_To_Hydrology 0.021683 19 sqrt_Euc_Distance_To_Hydrology 0.022279 4 Vertical_Distance_To_Hydrology 0.027925 9 Wilderness 0.038626 14 LC_Horizontal_Roadways_Vertical_Hydrology 0.038708 5 Horizontal_Distance_To_Roadways 0.038995 17 sqrt_Horizontal_Distance_To_Roadways 0.039787 13 LC_Horizontal_Roadways_Hydrology 0.041627 18 sqrt_Horizontal_Distance_To_Fire_Points 0.054596 8 Horizontal_Distance_To_Fire_Points 0.054826 12 LC_Horizontal_Roadways_FirePoints 0.068097 10 Soil 0.092398 15 sqrt_Elevation 0.141140 0 Elevation 0.141801 . Looking at the above chart as well as the correlation to Cover Type, I&#39;m going to drop the following columns: . Hillshade_Noon | Hillshade_9am | Horizontal_Distance_To_Hydrology | sqrt_Horizontal_Distance_To_Hydrology | Euc_Distance_To_Hydrology | Horizontal_Distance_To_Roadways | Aspect | . This is because other columns have a high correlation (and mututal information) with these columns, so they&#39;re redundant and will most likely create noise for our algorithms. . df = df.drop([ &#39;Hillshade_Noon&#39;,&#39;Hillshade_9am&#39;,&#39;Horizontal_Distance_To_Hydrology&#39;, &#39;sqrt_Horizontal_Distance_To_Hydrology&#39;,&#39;Euc_Distance_To_Hydrology&#39;, &#39;Horizontal_Distance_To_Roadways&#39; ,&#39;Aspect&#39;], axis = 1) . Let&#39;s see if there are any other columns left with very high correlation to other features. . plt.figure(figsize=(10,10)) sns.heatmap(df.corr(),cmap=&#39;BuGn&#39;,linecolor=&#39;white&#39;,linewidths=1,annot=True, xticklabels = True, yticklabels= True) . &lt;AxesSubplot:&gt; . Looks like there are a couple more columns we can remove. We will remove: . Elevation | Horizontal_Distance_To_Fire_Points | sqrt_Horizontal_Distance_To_Roadways | . df = df.drop([ &#39;sqrt_Horizontal_Distance_To_Roadways&#39;,&#39;Horizontal_Distance_To_Fire_Points&#39; ,&#39;Elevation&#39;, &#39;LC_Horizontal_Roadways_Vertical_Hydrology&#39; ], axis = 1) print(df.shape) . (575348, 13) . This is a good number of features, we can move on into other algorithms for prediction. . K-Nearest Neighbors (KNN) . To run some of our algorithms for classification, we will need to create a small function that can fit the model to the data, find the test accuracy, and append this info so we can visualize and compare models later. We also need to normalize the data with z-scores for some of the models, and especially K-NN, so we will do that pre-processing as well. This way, the model performs its best so we can evaluate the models. . def fit_evaluate_model(model, X_train, y_train, X_valid, Y_valid, method_name): &#39;&#39;&#39; This function trains a given model, generates predictions, appends the name of the method and the accuracy to a pre-defined list; then it returns accuracy rate Parameters: scikit-learn model, training data, training labels, test data, test labels Output: none; accuracy rate is printed &#39;&#39;&#39; model.fit(X_train, y_train) y_predicted = model.predict(X_valid) accuracy = accuracy_score(Y_valid, y_predicted) acc.append(accuracy) algs.append(method_name) print(&quot;Our {} algorithm classified {:0.2f}% of the of the trees correctly&quot;.format(method_name,accuracy*100)) . trees_tr = df.drop(&#39;Cover&#39;, axis = 1) labels_tr = df[&#39;Cover&#39;] train, test, train_labels, test_labels = train_test_split(trees_tr, labels_training, test_size=0.3, random_state=1) scaler = StandardScaler() # apply normalization to training set and transform training set train_scaled = scaler.fit_transform(train, train_labels) train_scaled = pd.DataFrame(train_scaled) # transform validation set test_scaled = scaler.transform(test) . knn = KNeighborsClassifier() fit_evaluate_model(knn, train_scaled, train_labels, test_scaled, test_labels, &#39;K-NN&#39;) . Our K-NN algorithm classified 92.71% of the of the trees correctly . Light Gradient Boosting Machine (LightGBM) . lgbm = LGBMClassifier() fit_evaluate_model(lgbm, train, train_labels, test, test_labels, &#39;LightGBM&#39;) . Our Gradient Boosting Machine algorithm classified 85.64% of the of the trees correctly . Extra Gradient Boosting (XGBoost) . xgb = XGBClassifier() fit_evaluate_model(xgb, train_scaled, train_labels, test_scaled, test_labels, &#39;XGBoost&#39;) . C: Users Patrick AppData Local Programs Python Python39 lib site-packages xgboost sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [15:08:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;multi:softprob&#39; was changed from &#39;merror&#39; to &#39;mlogloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. Our XGBoost algorithm classified 88.07% of the of the trees correctly . Comparing Models &amp; Conclusion . predictions = pd.DataFrame(zip(algs,acc), columns= [&#39;Model&#39;, &#39;Accuracy&#39;]) predictions = predictions.sort_values(by = &#39;Accuracy&#39;) plt.figure(figsize=(10,10)) ax = sns.barplot(x=&#39;Accuracy&#39;, y= &#39;Model&#39;, data=predictions, palette=&#39;Greens&#39;) plt.title(&quot;Prediction Accuracy of Different Models&quot;, size=14) . Text(0.5, 1.0, &#39;Prediction Accuracy of Different Models&#39;) . It seems the Random Forest algorithm was able to predict the tree cover most accurately. In fact, the accuracy is a bit higher for random forest after feature engineering, so random forest would be the algorithm of choice for predicting tree canopies. .",
            "url": "https://vrindachaa.github.io/wss/machine%20learning/python/jupyter/random%20forest/decision%20tree/xgboost/lgbm/extratrees/knn/scikit-learn/pandas/data%20science/2021/08/06/forestcovtype.html",
            "relUrl": "/machine%20learning/python/jupyter/random%20forest/decision%20tree/xgboost/lgbm/extratrees/knn/scikit-learn/pandas/data%20science/2021/08/06/forestcovtype.html",
            "date": "  Aug 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "MNIST Classification with Gradient Descent and its variants",
            "content": "by Vrinda Chauhan . Python libraries . We will use the following Python libraries to analyze this dataset: . pandas: dataframe data structure based on the the naive R data structure. Some advantages of using this library is that it utilizes vectorized processes for fast manipulation of the data. In terms of memory usage, however, pandas is much less efficient; its advantage lies in computations for datasets larger than 500,000 rows | numpy: python library that allows us to perform various mathematical operationss, especially for constructing and working with multidimensional arrays. numpy is better for memory usage for datasets smaller than 50,000 rows, and it allows use to use list comprehension instead of loops for our operations. | matplotlib: 2d plotting tools | IPython.display: displays the dataframes in a clean way | time: allows us to record and then calculate the speed of the different algorithms | . import pandas as pd import numpy as np import matplotlib.pyplot as plt from IPython.display import display, HTML from time import time %matplotlib inline . Discussion of the dataset . The MNIST dataset is a dataset of handwritten digits and their corresponding pixels. The data is split into two different datasets, one of the training data and one of the test data. . The first column of each data record is the label of the handwritten digit, a number from 0-9. The rest of the data record consists of the values of the pixels of the digit, a number between 0 and 255. This segment is a vector $x_{i} in R^{784}$, which are concatinated from an image of $28x28$ pixels of the given digit. . Let&#39;s see what this looks like. . train = pd.read_csv(&#39;./mnist_train.csv&#39;, header=None) . test = pd.read_csv(&#39;./mnist_test.csv&#39;, header=None) . print(&#39;The size of the training data is &#39;, train.shape) print(&#39;The size of the test data is &#39;, test.shape) . The size of the training data is (60000, 785) The size of the test data is (10000, 785) . display(HTML(train.head(15).to_html())) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 . 0 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 18 | 18 | 18 | 126 | 136 | 175 | 26 | 166 | 255 | 247 | 127 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 30 | 36 | 94 | 154 | 170 | 253 | 253 | 253 | 253 | 253 | 225 | 172 | 253 | 242 | 195 | 64 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49 | 238 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 251 | 93 | 82 | 82 | 56 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 18 | 219 | 253 | 253 | 253 | 253 | 253 | 198 | 182 | 247 | 241 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 80 | 156 | 107 | 253 | 253 | 205 | 11 | 0 | 43 | 154 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 1 | 154 | 253 | 90 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 139 | 253 | 190 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 190 | 253 | 70 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 35 | 241 | 225 | 160 | 108 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 81 | 240 | 253 | 253 | 119 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 186 | 253 | 253 | 150 | 27 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 93 | 252 | 253 | 187 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 249 | 64 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 46 | 130 | 183 | 253 | 253 | 207 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 39 | 148 | 229 | 253 | 253 | 253 | 250 | 182 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 24 | 114 | 221 | 253 | 253 | 253 | 253 | 201 | 78 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 23 | 66 | 213 | 253 | 253 | 253 | 253 | 198 | 81 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 18 | 171 | 219 | 253 | 253 | 253 | 253 | 195 | 80 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 55 | 172 | 226 | 253 | 253 | 253 | 253 | 244 | 133 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 136 | 253 | 253 | 253 | 212 | 135 | 132 | 16 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 51 | 159 | 253 | 159 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 238 | 252 | 252 | 252 | 237 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 54 | 227 | 253 | 252 | 239 | 233 | 252 | 57 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 10 | 60 | 224 | 252 | 253 | 252 | 202 | 84 | 252 | 253 | 122 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 163 | 252 | 252 | 252 | 253 | 252 | 252 | 96 | 189 | 253 | 167 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 51 | 238 | 253 | 253 | 190 | 114 | 253 | 228 | 47 | 79 | 255 | 168 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 238 | 252 | 252 | 179 | 12 | 75 | 121 | 21 | 0 | 0 | 253 | 243 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 165 | 253 | 233 | 208 | 84 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 165 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 178 | 252 | 240 | 71 | 19 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 195 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 57 | 252 | 252 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 195 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 198 | 253 | 190 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 253 | 196 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 76 | 246 | 252 | 112 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 148 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 252 | 230 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 135 | 253 | 186 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 252 | 223 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 131 | 252 | 225 | 71 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 252 | 145 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 165 | 252 | 173 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 86 | 253 | 225 | 0 | 0 | 0 | 0 | 0 | 0 | 114 | 238 | 253 | 162 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 252 | 249 | 146 | 48 | 29 | 85 | 178 | 225 | 253 | 223 | 167 | 56 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 252 | 252 | 252 | 229 | 215 | 252 | 252 | 252 | 196 | 130 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 28 | 199 | 252 | 252 | 253 | 252 | 252 | 233 | 145 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 25 | 128 | 252 | 253 | 252 | 141 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 67 | 232 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 62 | 81 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 120 | 180 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 126 | 163 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 153 | 210 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 220 | 163 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 27 | 254 | 162 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 222 | 163 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 183 | 254 | 125 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 46 | 245 | 163 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 198 | 254 | 56 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 120 | 254 | 163 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 23 | 231 | 254 | 29 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 159 | 254 | 120 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 163 | 254 | 216 | 16 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 159 | 254 | 67 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 86 | 178 | 248 | 254 | 91 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 159 | 254 | 85 | 0 | 0 | 0 | 47 | 49 | 116 | 144 | 150 | 241 | 243 | 234 | 179 | 241 | 252 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 150 | 253 | 237 | 207 | 207 | 207 | 253 | 254 | 250 | 240 | 198 | 143 | 91 | 28 | 5 | 233 | 250 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 119 | 177 | 177 | 177 | 177 | 177 | 98 | 56 | 0 | 0 | 0 | 0 | 0 | 102 | 254 | 220 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 254 | 137 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 254 | 57 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 254 | 57 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 255 | 94 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 254 | 96 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 254 | 153 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 255 | 153 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 96 | 254 | 153 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 124 | 253 | 255 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 96 | 244 | 251 | 253 | 62 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 127 | 251 | 251 | 253 | 62 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 68 | 236 | 251 | 211 | 31 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 60 | 228 | 251 | 251 | 94 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 155 | 253 | 253 | 189 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 253 | 251 | 235 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 205 | 253 | 251 | 126 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 104 | 251 | 253 | 184 | 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 80 | 240 | 251 | 193 | 23 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 253 | 253 | 253 | 159 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 151 | 251 | 251 | 251 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 221 | 251 | 251 | 172 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 234 | 251 | 251 | 196 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 251 | 251 | 89 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 159 | 255 | 253 | 253 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 228 | 253 | 247 | 140 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 64 | 251 | 253 | 220 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 64 | 251 | 253 | 220 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 24 | 193 | 253 | 220 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 55 | 148 | 210 | 253 | 253 | 113 | 87 | 148 | 55 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 87 | 232 | 252 | 253 | 189 | 210 | 252 | 252 | 253 | 168 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 57 | 242 | 252 | 190 | 65 | 5 | 12 | 182 | 252 | 253 | 116 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 96 | 252 | 252 | 183 | 14 | 0 | 0 | 92 | 252 | 252 | 225 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 132 | 253 | 252 | 146 | 14 | 0 | 0 | 0 | 215 | 252 | 252 | 79 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 126 | 253 | 247 | 176 | 9 | 0 | 0 | 8 | 78 | 245 | 253 | 129 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 232 | 252 | 176 | 0 | 0 | 0 | 36 | 201 | 252 | 252 | 169 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 22 | 252 | 252 | 30 | 22 | 119 | 197 | 241 | 253 | 252 | 251 | 77 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 231 | 252 | 253 | 252 | 252 | 252 | 226 | 227 | 252 | 231 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 55 | 235 | 253 | 217 | 138 | 42 | 24 | 192 | 252 | 143 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 62 | 255 | 253 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 71 | 253 | 252 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 71 | 253 | 252 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 106 | 253 | 252 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 255 | 253 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 218 | 252 | 56 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 96 | 252 | 189 | 42 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 184 | 252 | 170 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 147 | 252 | 42 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 25 | 100 | 122 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 33 | 151 | 208 | 252 | 252 | 252 | 146 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 40 | 152 | 244 | 252 | 253 | 224 | 211 | 252 | 232 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 152 | 239 | 252 | 252 | 252 | 216 | 31 | 37 | 252 | 252 | 60 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 96 | 252 | 252 | 252 | 252 | 217 | 29 | 0 | 37 | 252 | 252 | 60 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 181 | 252 | 252 | 220 | 167 | 30 | 0 | 0 | 77 | 252 | 252 | 60 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 26 | 128 | 58 | 22 | 0 | 0 | 0 | 0 | 100 | 252 | 252 | 60 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 157 | 252 | 252 | 60 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 110 | 121 | 122 | 121 | 202 | 252 | 194 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 10 | 53 | 179 | 253 | 253 | 255 | 253 | 253 | 228 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 54 | 227 | 252 | 243 | 228 | 170 | 242 | 252 | 252 | 231 | 117 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 78 | 252 | 252 | 125 | 59 | 0 | 18 | 208 | 252 | 252 | 252 | 252 | 87 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 135 | 252 | 252 | 180 | 16 | 0 | 21 | 203 | 253 | 247 | 129 | 173 | 252 | 252 | 184 | 66 | 49 | 49 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 136 | 252 | 241 | 106 | 17 | 0 | 53 | 200 | 252 | 216 | 65 | 0 | 14 | 72 | 163 | 241 | 252 | 252 | 223 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 105 | 252 | 242 | 88 | 18 | 73 | 170 | 244 | 252 | 126 | 29 | 0 | 0 | 0 | 0 | 0 | 89 | 180 | 180 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 231 | 252 | 245 | 205 | 216 | 252 | 252 | 252 | 124 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 207 | 252 | 252 | 252 | 252 | 178 | 116 | 36 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 93 | 143 | 121 | 23 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 145 | 255 | 211 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 237 | 253 | 252 | 71 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 175 | 253 | 252 | 71 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 144 | 253 | 252 | 71 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 191 | 253 | 252 | 71 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 26 | 221 | 253 | 252 | 124 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 125 | 253 | 252 | 252 | 108 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 252 | 108 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 253 | 253 | 108 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 252 | 108 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 252 | 108 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 252 | 108 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 253 | 253 | 170 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 252 | 252 | 42 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 149 | 252 | 252 | 252 | 144 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 109 | 252 | 252 | 252 | 144 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 218 | 253 | 253 | 255 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 175 | 252 | 252 | 253 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 73 | 252 | 252 | 253 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 211 | 252 | 253 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 43 | 105 | 255 | 253 | 253 | 253 | 253 | 253 | 174 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 139 | 224 | 226 | 252 | 253 | 252 | 252 | 252 | 252 | 252 | 252 | 158 | 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 178 | 252 | 252 | 252 | 252 | 253 | 252 | 252 | 252 | 252 | 252 | 252 | 252 | 59 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 109 | 252 | 252 | 230 | 132 | 133 | 132 | 132 | 189 | 252 | 252 | 252 | 252 | 59 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 29 | 29 | 24 | 0 | 0 | 0 | 0 | 14 | 226 | 252 | 252 | 172 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 243 | 252 | 252 | 144 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 88 | 189 | 252 | 252 | 252 | 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 91 | 212 | 247 | 252 | 252 | 252 | 204 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 125 | 193 | 193 | 193 | 253 | 252 | 252 | 252 | 238 | 102 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 222 | 252 | 252 | 252 | 252 | 253 | 252 | 252 | 252 | 177 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 223 | 253 | 253 | 253 | 253 | 255 | 253 | 253 | 253 | 253 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 123 | 52 | 44 | 44 | 44 | 44 | 143 | 252 | 252 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 252 | 252 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 86 | 252 | 252 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 75 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 242 | 252 | 252 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 61 | 183 | 252 | 29 | 0 | 0 | 0 | 0 | 18 | 92 | 239 | 252 | 252 | 243 | 65 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 208 | 252 | 252 | 147 | 134 | 134 | 134 | 134 | 203 | 253 | 252 | 252 | 188 | 83 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 208 | 252 | 252 | 252 | 252 | 252 | 252 | 252 | 252 | 253 | 230 | 153 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49 | 157 | 252 | 252 | 252 | 252 | 252 | 217 | 207 | 146 | 45 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 103 | 235 | 252 | 172 | 103 | 24 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 63 | 197 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 254 | 230 | 24 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 254 | 254 | 48 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 254 | 255 | 48 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 254 | 254 | 57 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 254 | 254 | 108 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 239 | 254 | 143 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 178 | 254 | 143 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 178 | 254 | 143 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 178 | 254 | 162 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 178 | 254 | 240 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 113 | 254 | 240 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 83 | 254 | 245 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 79 | 254 | 246 | 38 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 214 | 254 | 150 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 144 | 241 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 144 | 240 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 144 | 254 | 82 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 230 | 247 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 168 | 209 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 189 | 190 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 143 | 247 | 153 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 136 | 247 | 242 | 86 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 192 | 252 | 187 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 62 | 185 | 18 | 0 | 0 | 0 | 0 | 89 | 236 | 217 | 47 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 216 | 253 | 60 | 0 | 0 | 0 | 0 | 212 | 255 | 81 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 206 | 252 | 68 | 0 | 0 | 0 | 48 | 242 | 253 | 89 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 131 | 251 | 212 | 21 | 0 | 0 | 11 | 167 | 252 | 197 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 232 | 247 | 63 | 0 | 0 | 0 | 153 | 252 | 226 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 219 | 252 | 143 | 0 | 0 | 0 | 116 | 249 | 252 | 103 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 96 | 253 | 255 | 253 | 200 | 122 | 7 | 25 | 201 | 250 | 158 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 92 | 252 | 252 | 253 | 217 | 252 | 252 | 200 | 227 | 252 | 231 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 87 | 251 | 247 | 231 | 65 | 48 | 189 | 252 | 252 | 253 | 252 | 251 | 227 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 190 | 221 | 98 | 0 | 0 | 0 | 42 | 196 | 252 | 253 | 252 | 252 | 162 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 111 | 29 | 0 | 0 | 0 | 0 | 62 | 239 | 252 | 86 | 42 | 42 | 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 148 | 253 | 218 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 121 | 252 | 231 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 221 | 251 | 129 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 218 | 252 | 160 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 122 | 252 | 82 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | 118 | 219 | 166 | 118 | 118 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 103 | 242 | 254 | 254 | 254 | 254 | 254 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 18 | 232 | 254 | 254 | 254 | 254 | 254 | 238 | 70 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 104 | 244 | 254 | 224 | 254 | 254 | 254 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 207 | 254 | 210 | 254 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 206 | 254 | 254 | 254 | 254 | 41 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 24 | 209 | 254 | 254 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 91 | 137 | 253 | 254 | 254 | 254 | 112 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 40 | 214 | 250 | 254 | 254 | 254 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 81 | 247 | 254 | 254 | 254 | 254 | 254 | 254 | 146 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 110 | 246 | 254 | 254 | 254 | 254 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 73 | 89 | 89 | 93 | 240 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 128 | 254 | 219 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 254 | 254 | 214 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 138 | 254 | 254 | 116 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 19 | 177 | 90 | 0 | 0 | 0 | 0 | 0 | 25 | 240 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 164 | 254 | 215 | 63 | 36 | 0 | 51 | 89 | 206 | 254 | 254 | 139 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 57 | 197 | 254 | 254 | 222 | 180 | 241 | 254 | 254 | 253 | 213 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 140 | 105 | 254 | 254 | 254 | 254 | 254 | 254 | 236 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 117 | 117 | 165 | 254 | 254 | 239 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 40 | 129 | 234 | 234 | 159 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 68 | 150 | 239 | 254 | 253 | 253 | 253 | 215 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 156 | 201 | 254 | 254 | 254 | 241 | 150 | 98 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 19 | 154 | 254 | 236 | 203 | 83 | 39 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 144 | 253 | 145 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 10 | 129 | 222 | 78 | 79 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 134 | 253 | 167 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 254 | 78 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 201 | 253 | 226 | 69 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 55 | 6 | 0 | 18 | 128 | 253 | 241 | 41 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 25 | 205 | 235 | 92 | 0 | 0 | 20 | 253 | 253 | 58 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 231 | 245 | 108 | 0 | 0 | 0 | 132 | 253 | 185 | 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 121 | 245 | 254 | 254 | 254 | 217 | 254 | 223 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 116 | 165 | 233 | 233 | 234 | 180 | 39 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 12 | 99 | 91 | 142 | 155 | 246 | 182 | 155 | 155 | 155 | 155 | 131 | 52 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 138 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 252 | 210 | 122 | 33 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 220 | 254 | 254 | 254 | 235 | 189 | 189 | 189 | 189 | 150 | 189 | 205 | 254 | 254 | 254 | 75 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 35 | 74 | 35 | 35 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 224 | 254 | 254 | 153 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 90 | 254 | 254 | 247 | 53 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 152 | 246 | 254 | 254 | 49 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 66 | 158 | 254 | 254 | 249 | 103 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 54 | 251 | 254 | 254 | 254 | 248 | 74 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 140 | 254 | 254 | 254 | 254 | 254 | 254 | 202 | 125 | 45 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 58 | 181 | 234 | 254 | 254 | 254 | 254 | 254 | 254 | 252 | 140 | 22 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 30 | 50 | 73 | 155 | 253 | 254 | 254 | 254 | 254 | 191 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 91 | 200 | 254 | 254 | 254 | 254 | 118 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 192 | 254 | 254 | 254 | 154 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 141 | 254 | 254 | 254 | 116 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 25 | 126 | 86 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 188 | 254 | 254 | 250 | 61 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 24 | 209 | 254 | 15 | 0 | 0 | 0 | 0 | 0 | 23 | 137 | 254 | 254 | 254 | 209 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 168 | 254 | 254 | 48 | 9 | 0 | 0 | 9 | 127 | 241 | 254 | 254 | 255 | 242 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 101 | 254 | 254 | 254 | 205 | 190 | 190 | 205 | 254 | 254 | 254 | 254 | 242 | 67 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 33 | 166 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 250 | 138 | 55 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 88 | 154 | 116 | 194 | 194 | 154 | 154 | 88 | 49 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 222 | 225 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 147 | 234 | 252 | 176 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 23 | 197 | 253 | 252 | 208 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 178 | 252 | 253 | 117 | 65 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 57 | 252 | 252 | 253 | 89 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 222 | 253 | 253 | 79 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 131 | 252 | 179 | 27 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 198 | 246 | 220 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 79 | 253 | 252 | 135 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 140 | 253 | 252 | 118 | 0 | 0 | 0 | 0 | 111 | 140 | 140 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 191 | 255 | 253 | 56 | 0 | 0 | 114 | 113 | 222 | 253 | 253 | 255 | 27 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 76 | 252 | 253 | 223 | 37 | 0 | 48 | 174 | 252 | 252 | 242 | 214 | 253 | 199 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 109 | 252 | 228 | 130 | 0 | 38 | 165 | 253 | 233 | 164 | 49 | 63 | 253 | 214 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 73 | 252 | 252 | 126 | 0 | 23 | 178 | 252 | 240 | 148 | 7 | 44 | 215 | 240 | 148 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 119 | 252 | 252 | 0 | 0 | 197 | 252 | 252 | 63 | 0 | 57 | 252 | 252 | 140 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 135 | 253 | 174 | 0 | 48 | 229 | 253 | 112 | 0 | 38 | 222 | 253 | 112 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 135 | 252 | 173 | 0 | 48 | 227 | 252 | 158 | 226 | 234 | 201 | 27 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 57 | 252 | 252 | 57 | 104 | 240 | 252 | 252 | 253 | 233 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 51 | 242 | 252 | 253 | 252 | 252 | 252 | 252 | 240 | 148 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 75 | 189 | 253 | 252 | 252 | 157 | 112 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 168 | 242 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 10 | 228 | 254 | 100 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 190 | 254 | 122 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 83 | 254 | 162 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 248 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 255 | 254 | 103 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 255 | 254 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 254 | 254 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 212 | 254 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 203 | 254 | 178 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 155 | 254 | 190 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 199 | 104 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . display(HTML(test.head(15).to_html())) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 . 0 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 185 | 159 | 151 | 60 | 36 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 222 | 254 | 254 | 254 | 254 | 241 | 198 | 198 | 198 | 198 | 198 | 198 | 198 | 198 | 170 | 52 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 67 | 114 | 72 | 114 | 163 | 227 | 254 | 225 | 254 | 254 | 254 | 250 | 229 | 254 | 254 | 140 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 66 | 14 | 67 | 67 | 67 | 59 | 21 | 236 | 254 | 106 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 83 | 253 | 209 | 18 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 22 | 233 | 255 | 83 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 129 | 254 | 238 | 44 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 59 | 249 | 254 | 62 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 133 | 254 | 187 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 205 | 248 | 58 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 126 | 254 | 182 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 75 | 251 | 240 | 57 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 19 | 221 | 254 | 166 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 203 | 254 | 219 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 254 | 254 | 77 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 224 | 254 | 115 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 133 | 254 | 254 | 52 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 61 | 242 | 254 | 254 | 52 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 121 | 254 | 254 | 219 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 121 | 254 | 207 | 18 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 116 | 125 | 171 | 255 | 255 | 150 | 93 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 253 | 253 | 253 | 253 | 253 | 253 | 218 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 253 | 253 | 253 | 213 | 142 | 176 | 253 | 253 | 122 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 52 | 250 | 253 | 210 | 32 | 12 | 0 | 6 | 206 | 253 | 140 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 77 | 251 | 210 | 25 | 0 | 0 | 0 | 122 | 248 | 253 | 65 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 18 | 0 | 0 | 0 | 0 | 209 | 253 | 253 | 65 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 117 | 247 | 253 | 198 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 76 | 247 | 253 | 231 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 128 | 253 | 253 | 144 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 176 | 246 | 253 | 159 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 25 | 234 | 253 | 233 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 198 | 253 | 253 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 78 | 248 | 253 | 189 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 19 | 200 | 253 | 253 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 134 | 253 | 253 | 173 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 248 | 253 | 253 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 248 | 253 | 253 | 43 | 20 | 20 | 20 | 20 | 5 | 0 | 5 | 20 | 20 | 37 | 150 | 150 | 150 | 147 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 248 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 168 | 143 | 166 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 123 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 174 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 249 | 247 | 247 | 169 | 117 | 117 | 57 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 118 | 123 | 123 | 123 | 166 | 253 | 253 | 253 | 155 | 123 | 123 | 41 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 254 | 109 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 87 | 252 | 82 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 135 | 241 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 244 | 150 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 254 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 202 | 223 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 254 | 216 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 95 | 254 | 195 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 140 | 254 | 77 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 57 | 237 | 205 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 124 | 255 | 165 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 171 | 254 | 81 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 24 | 232 | 215 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 120 | 254 | 159 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 151 | 254 | 142 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 228 | 254 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 61 | 251 | 254 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 141 | 254 | 205 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 10 | 215 | 254 | 121 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 198 | 176 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 150 | 253 | 202 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 37 | 251 | 251 | 253 | 107 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 197 | 251 | 251 | 253 | 107 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 110 | 190 | 251 | 251 | 251 | 253 | 169 | 109 | 62 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 251 | 251 | 251 | 251 | 253 | 251 | 251 | 220 | 51 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 182 | 255 | 253 | 253 | 253 | 253 | 234 | 222 | 253 | 253 | 253 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 63 | 221 | 253 | 251 | 251 | 251 | 147 | 77 | 62 | 128 | 251 | 251 | 105 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 231 | 251 | 253 | 251 | 220 | 137 | 10 | 0 | 0 | 31 | 230 | 251 | 243 | 113 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 37 | 251 | 251 | 253 | 188 | 20 | 0 | 0 | 0 | 0 | 0 | 109 | 251 | 253 | 251 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 37 | 251 | 251 | 201 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 200 | 253 | 251 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 37 | 253 | 253 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 202 | 255 | 253 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 140 | 251 | 251 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 109 | 251 | 253 | 251 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 217 | 251 | 251 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 63 | 231 | 251 | 253 | 230 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 217 | 251 | 251 | 0 | 0 | 0 | 0 | 0 | 0 | 144 | 251 | 251 | 251 | 221 | 61 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 217 | 251 | 251 | 0 | 0 | 0 | 0 | 0 | 182 | 221 | 251 | 251 | 251 | 180 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 218 | 253 | 253 | 73 | 73 | 228 | 253 | 253 | 255 | 253 | 253 | 253 | 253 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 113 | 251 | 251 | 253 | 251 | 251 | 251 | 251 | 253 | 251 | 251 | 251 | 147 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 230 | 251 | 253 | 251 | 251 | 251 | 251 | 253 | 230 | 189 | 35 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 62 | 142 | 253 | 251 | 251 | 251 | 251 | 253 | 107 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 72 | 174 | 251 | 173 | 71 | 72 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 50 | 224 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 70 | 29 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 121 | 231 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 148 | 168 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 195 | 231 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 96 | 210 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 69 | 252 | 134 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 114 | 252 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 236 | 217 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 192 | 252 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 168 | 247 | 53 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 18 | 255 | 253 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 242 | 211 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 141 | 253 | 189 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 252 | 106 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 232 | 250 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 225 | 252 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 134 | 252 | 211 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 22 | 252 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 252 | 167 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 204 | 209 | 18 | 0 | 0 | 0 | 0 | 0 | 0 | 22 | 253 | 253 | 107 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 252 | 199 | 85 | 85 | 85 | 85 | 129 | 164 | 195 | 252 | 252 | 106 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 41 | 170 | 245 | 252 | 252 | 252 | 252 | 232 | 231 | 251 | 252 | 252 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49 | 84 | 84 | 84 | 84 | 0 | 0 | 161 | 252 | 252 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 127 | 252 | 252 | 45 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 128 | 253 | 253 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 127 | 252 | 252 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 135 | 252 | 244 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 232 | 236 | 111 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 179 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 77 | 254 | 107 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 19 | 227 | 254 | 254 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 81 | 254 | 254 | 165 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 203 | 254 | 254 | 73 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 53 | 254 | 254 | 250 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 134 | 254 | 254 | 180 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 196 | 254 | 248 | 48 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 58 | 254 | 254 | 237 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 111 | 254 | 254 | 132 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 163 | 254 | 238 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 60 | 252 | 254 | 223 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 79 | 254 | 254 | 154 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 163 | 254 | 238 | 53 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 28 | 252 | 254 | 210 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 86 | 254 | 254 | 131 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 105 | 254 | 234 | 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 175 | 254 | 204 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 211 | 254 | 196 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 158 | 254 | 160 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 26 | 157 | 107 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 22 | 192 | 134 | 32 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 77 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 235 | 250 | 169 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 220 | 241 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 189 | 253 | 147 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 139 | 253 | 100 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 70 | 253 | 253 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 254 | 173 | 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 22 | 153 | 253 | 96 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 231 | 254 | 92 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 163 | 255 | 204 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 104 | 254 | 158 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 162 | 253 | 178 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 131 | 237 | 253 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 162 | 253 | 253 | 191 | 175 | 70 | 70 | 70 | 70 | 133 | 197 | 253 | 253 | 169 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 51 | 228 | 253 | 253 | 254 | 253 | 253 | 253 | 253 | 254 | 253 | 253 | 219 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 65 | 137 | 254 | 232 | 137 | 137 | 137 | 44 | 253 | 253 | 161 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 | 254 | 206 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 160 | 253 | 69 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 254 | 241 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 158 | 254 | 165 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 231 | 244 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 104 | 254 | 232 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 208 | 253 | 157 | 0 | 13 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 208 | 253 | 154 | 91 | 204 | 161 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 208 | 253 | 254 | 253 | 154 | 29 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 61 | 190 | 128 | 23 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 149 | 193 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 91 | 224 | 253 | 253 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 28 | 235 | 254 | 253 | 253 | 166 | 18 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 144 | 253 | 254 | 253 | 253 | 253 | 238 | 115 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 241 | 253 | 208 | 185 | 253 | 253 | 253 | 231 | 24 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 79 | 254 | 193 | 0 | 8 | 98 | 219 | 254 | 255 | 201 | 18 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 86 | 253 | 80 | 0 | 0 | 0 | 182 | 253 | 254 | 191 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 175 | 253 | 155 | 0 | 0 | 0 | 234 | 253 | 254 | 135 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 86 | 253 | 208 | 40 | 85 | 166 | 251 | 237 | 254 | 236 | 42 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 18 | 238 | 253 | 254 | 253 | 253 | 185 | 36 | 216 | 253 | 152 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 68 | 240 | 255 | 254 | 145 | 8 | 0 | 134 | 254 | 223 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 68 | 158 | 142 | 12 | 0 | 0 | 9 | 175 | 253 | 161 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 88 | 253 | 226 | 18 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 166 | 253 | 126 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 245 | 253 | 38 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 115 | 254 | 172 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 218 | 254 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 30 | 254 | 165 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 186 | 244 | 42 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 223 | 78 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 47 | 47 | 47 | 16 | 129 | 85 | 47 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 75 | 153 | 217 | 253 | 253 | 253 | 215 | 246 | 253 | 253 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 35 | 142 | 244 | 252 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 63 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 213 | 170 | 170 | 170 | 170 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 132 | 72 | 0 | 57 | 238 | 227 | 238 | 168 | 124 | 69 | 20 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 206 | 253 | 78 | 0 | 0 | 32 | 0 | 30 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 177 | 253 | 132 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 12 | 133 | 253 | 233 | 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 92 | 253 | 223 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 150 | 253 | 174 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 234 | 253 | 246 | 127 | 49 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 253 | 253 | 253 | 251 | 147 | 91 | 121 | 85 | 42 | 42 | 85 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 139 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 232 | 168 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 53 | 218 | 222 | 251 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 252 | 124 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 67 | 72 | 200 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 175 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 120 | 253 | 249 | 152 | 51 | 164 | 253 | 253 | 175 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 50 | 253 | 253 | 253 | 188 | 252 | 253 | 253 | 148 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 167 | 253 | 253 | 253 | 253 | 250 | 175 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 23 | 180 | 231 | 253 | 221 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 93 | 149 | 22 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 36 | 56 | 137 | 201 | 199 | 95 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 152 | 234 | 254 | 254 | 254 | 254 | 254 | 250 | 211 | 151 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 46 | 153 | 240 | 254 | 254 | 227 | 166 | 133 | 251 | 200 | 254 | 229 | 225 | 104 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 153 | 234 | 254 | 254 | 187 | 142 | 8 | 0 | 0 | 191 | 40 | 198 | 246 | 223 | 253 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8 | 126 | 253 | 254 | 233 | 128 | 11 | 0 | 0 | 0 | 0 | 210 | 43 | 70 | 254 | 254 | 254 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 72 | 243 | 254 | 228 | 54 | 0 | 0 | 0 | 0 | 3 | 32 | 116 | 225 | 242 | 254 | 255 | 162 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 75 | 240 | 254 | 223 | 109 | 138 | 178 | 178 | 169 | 210 | 251 | 231 | 254 | 254 | 254 | 232 | 38 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 175 | 244 | 253 | 255 | 254 | 254 | 251 | 254 | 254 | 254 | 254 | 254 | 252 | 171 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 136 | 195 | 176 | 146 | 153 | 200 | 254 | 254 | 254 | 254 | 150 | 16 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 162 | 254 | 254 | 241 | 99 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 118 | 250 | 254 | 254 | 90 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 100 | 242 | 254 | 254 | 211 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 54 | 241 | 254 | 254 | 242 | 59 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 131 | 254 | 254 | 244 | 64 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 249 | 254 | 254 | 152 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 12 | 228 | 254 | 254 | 208 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 78 | 255 | 254 | 254 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 209 | 254 | 254 | 137 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 227 | 255 | 233 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 113 | 255 | 108 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 61 | 3 | 42 | 118 | 193 | 118 | 118 | 61 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 179 | 245 | 236 | 242 | 254 | 254 | 254 | 254 | 245 | 235 | 84 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 151 | 254 | 254 | 254 | 213 | 192 | 178 | 178 | 180 | 254 | 254 | 241 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 235 | 254 | 226 | 64 | 28 | 12 | 0 | 0 | 2 | 128 | 252 | 255 | 173 | 17 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 56 | 254 | 253 | 107 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 134 | 250 | 254 | 75 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 63 | 254 | 158 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 221 | 254 | 157 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 194 | 254 | 103 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 150 | 254 | 213 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 | 220 | 239 | 58 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 254 | 213 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 126 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 254 | 213 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 214 | 239 | 60 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 254 | 213 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 214 | 199 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 254 | 213 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 219 | 199 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 254 | 213 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 254 | 199 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 162 | 254 | 209 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 254 | 199 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 51 | 238 | 254 | 75 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 254 | 199 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 51 | 165 | 254 | 195 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 66 | 241 | 199 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 167 | 254 | 227 | 55 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 214 | 213 | 20 | 0 | 0 | 0 | 0 | 0 | 46 | 152 | 202 | 254 | 254 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 214 | 254 | 204 | 180 | 180 | 180 | 180 | 180 | 235 | 254 | 254 | 234 | 156 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 81 | 205 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 252 | 234 | 120 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 26 | 210 | 254 | 254 | 254 | 254 | 254 | 153 | 104 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 204 | 253 | 176 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 150 | 252 | 252 | 125 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 117 | 252 | 186 | 56 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 141 | 252 | 118 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 154 | 247 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 26 | 253 | 196 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 150 | 253 | 196 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 57 | 85 | 85 | 38 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 225 | 253 | 96 | 0 | 0 | 0 | 0 | 0 | 151 | 226 | 243 | 252 | 252 | 238 | 125 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 10 | 229 | 226 | 0 | 0 | 0 | 4 | 54 | 229 | 253 | 255 | 234 | 175 | 225 | 255 | 228 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 110 | 252 | 150 | 0 | 0 | 26 | 128 | 252 | 252 | 227 | 134 | 28 | 0 | 0 | 178 | 252 | 56 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 159 | 252 | 113 | 0 | 0 | 150 | 253 | 252 | 186 | 43 | 0 | 0 | 0 | 0 | 141 | 252 | 56 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 185 | 252 | 113 | 0 | 38 | 237 | 253 | 151 | 6 | 0 | 0 | 0 | 0 | 0 | 141 | 202 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 198 | 253 | 114 | 0 | 147 | 253 | 163 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 154 | 197 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 197 | 252 | 113 | 0 | 172 | 252 | 188 | 0 | 0 | 0 | 0 | 0 | 0 | 26 | 253 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 197 | 252 | 113 | 0 | 19 | 231 | 247 | 122 | 19 | 0 | 0 | 0 | 0 | 200 | 244 | 56 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 26 | 222 | 252 | 113 | 0 | 0 | 25 | 203 | 252 | 193 | 13 | 0 | 76 | 200 | 249 | 125 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 185 | 253 | 179 | 10 | 0 | 0 | 0 | 76 | 35 | 29 | 154 | 253 | 244 | 125 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 28 | 209 | 253 | 196 | 82 | 57 | 57 | 131 | 197 | 252 | 253 | 214 | 81 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 25 | 216 | 252 | 252 | 252 | 253 | 252 | 252 | 252 | 156 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 103 | 139 | 240 | 140 | 139 | 139 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49 | 180 | 253 | 255 | 253 | 169 | 36 | 11 | 76 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 68 | 228 | 252 | 252 | 253 | 252 | 252 | 160 | 189 | 253 | 92 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 55 | 252 | 252 | 227 | 79 | 69 | 69 | 100 | 90 | 236 | 247 | 67 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 233 | 252 | 185 | 50 | 0 | 0 | 0 | 26 | 203 | 252 | 135 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 168 | 253 | 178 | 37 | 0 | 0 | 0 | 0 | 70 | 252 | 252 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 155 | 253 | 242 | 42 | 0 | 0 | 0 | 0 | 5 | 191 | 253 | 190 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 207 | 252 | 230 | 0 | 0 | 0 | 0 | 5 | 136 | 252 | 252 | 64 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 207 | 252 | 230 | 0 | 0 | 0 | 32 | 138 | 252 | 252 | 227 | 16 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 165 | 252 | 249 | 207 | 207 | 207 | 228 | 253 | 252 | 252 | 160 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 179 | 253 | 252 | 252 | 252 | 252 | 75 | 169 | 252 | 56 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 64 | 116 | 116 | 74 | 0 | 149 | 253 | 215 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 253 | 252 | 162 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 253 | 240 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 157 | 253 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 240 | 253 | 92 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 93 | 253 | 252 | 84 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 114 | 252 | 209 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 207 | 252 | 116 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 165 | 252 | 116 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 93 | 200 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 66 | 138 | 255 | 253 | 169 | 138 | 23 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 120 | 228 | 252 | 252 | 253 | 252 | 252 | 252 | 158 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 108 | 252 | 252 | 252 | 252 | 190 | 252 | 252 | 252 | 252 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 233 | 252 | 252 | 252 | 116 | 5 | 135 | 252 | 252 | 252 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 178 | 253 | 252 | 221 | 43 | 2 | 0 | 5 | 54 | 232 | 252 | 210 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 93 | 253 | 255 | 249 | 115 | 0 | 0 | 0 | 0 | 0 | 136 | 251 | 255 | 154 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 166 | 252 | 253 | 185 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 209 | 253 | 206 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 19 | 220 | 252 | 253 | 92 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 116 | 253 | 206 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 70 | 252 | 252 | 192 | 17 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 116 | 253 | 223 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 122 | 252 | 252 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 116 | 253 | 252 | 69 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 132 | 253 | 253 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 116 | 255 | 253 | 69 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 184 | 252 | 252 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 116 | 253 | 252 | 69 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 184 | 252 | 252 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 116 | 253 | 240 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 184 | 252 | 252 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 210 | 253 | 112 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 232 | 252 | 158 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 230 | 232 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 93 | 253 | 244 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 155 | 253 | 168 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 | 164 | 253 | 113 | 0 | 0 | 0 | 0 | 0 | 66 | 236 | 231 | 42 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 222 | 240 | 134 | 0 | 0 | 38 | 91 | 234 | 252 | 137 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 25 | 177 | 240 | 207 | 103 | 233 | 252 | 252 | 176 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 54 | 179 | 252 | 137 | 137 | 54 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 128 | 255 | 191 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 191 | 255 | 255 | 64 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 128 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 255 | 255 | 255 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 191 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 191 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 64 | 255 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 191 | 255 | 255 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 64 | 255 | 255 | 255 | 64 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 64 | 255 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Note the data will be normalized, ie divided by 255 so that each pixel entry will be a real number in $[0,1]$; this is mainly to simplify the mathematical calculations in the gradient or loss, for example. . Visualizing MNIST Digits . We mentioned earlier that each data record has a label and a concatinated pixel vector. Let&#39;s try to visualize what a randomly chosen digit from the training set will look like for each digit. To do this, we can define the following helper function: . def visualize(df, i): &#39;&#39;&#39;&#39; This method helps us visualize the digits from their concatinated vector as a row of the dataset into the image it represents. Parameters: a dataset df, a desired digit we want to visualize, i Output: a plot with the digit and the corresponding index, chosen randomly &#39;&#39;&#39; a = df[df.columns[0]] label = list((float(i) for i in (a))) # n is our on and off switch for the while loop n = 0 # the while loop picks a randomly indexed data record chosen by the np.random.choice method and compares its label # to our desired i; if it is our desired digit then we engage in plotting it. If not, we keep remain engaged in the # while loop to keep picking random indexes while (n == 0): a_choice = np.random.choice(range(n, len(label))) if i == label[a_choice]: a_pixels = df.loc[a_choice] a_pixels = np.array(a_pixels[1:], dtype=&#39;uint8&#39;) a_pixels = a_pixels.reshape((28, 28)) plt.imshow(a_pixels, cmap=&#39;gray&#39;) plt.title(&#39;Label is {i}, Index is {a_choice}&#39;.format( i=i, a_choice=a_choice)) plt.show() n = 1 else: n = 0 . visualize(test, 0) visualize(test, 1) visualize(test, 2) visualize(test, 3) visualize(test, 4) visualize(train, 5) visualize(train, 6) visualize(train, 7) visualize(train, 8) visualize(train, 9) . Discussion of the problem . With this dataset, we want to solve a classification problem using discriminant analysis; that is, we want an algorithm to be able to recognize the digit of an input pixel vector between a set of two digits in 0-9. We want to use the training sample to find a vector such that when we take the cross product of a data record&#39;s pixel segment and a derived weight vector, we can predict which digit the pixel vector represents between two given digits (in the algorithm, we correspond each of these digits with 1, -1 to make this calculation and function easier). In mathematical terms, we want $w$ such that $$ y_{i} = sign(w^{T}x_{i}) $$ where y is the label for the $i^{th}$ data record and the $x_{i}$ is the vector of pixels for that digit. Intuitively, what we are looking for the best fitting hyperplane that serperates the class of one digit&#39;s pixels from the class of the other digit&#39;s pixels. This will allow us to have a vector that makes predictions for the test data. In 2-D (ie if the data only had two columns), this idea can be illustrated by the following image: Here, the dots are 2-d datapoints, and we want to find the line that best seperates the two clusters of data. Recall the function of a line is $ y = mx + b$, so in 2-D, our &quot;weight vector&quot; is the variable $m$, and we would want to pick or &quot;learn&quot; an $m$ so that the distances of the dotted lines are minimized. . Since our dataset has 784 columns, we want to try to learn a length 784 weight vector so that we can seperate the two classes of data and make predictions on the test set. Using this, we can understand and compare the performance of each model to assess the accuracy, speed, and computational complexity of the model, and understand the different tradeoffs made in each method. We will use the gradient descent algorithm and its variations to solve this problem. . Discussion of the various Gradient Descent Methods . Discussion of the loss function and some probability . In order to understand the gradient descent algorithm, we must first understand the distribution of the dataset, and the loss function affiliated with it. A loss function calculates the error rate for the classification. Our goal is to minimize this loss function. . In the MNIST dataset, we will be using a loss function called cross-entropy loss, which we will derive below. . It can be shown the cumilitive distribution function of $G_{z}(z) = frac{1}{1+e^{-z}} $ can model the probability of a given digit being the correct class label, if we affiliate our labels with a -1 and 1 (for example, if our model was meant to decide if a set of pixels is a 4 or 9, we could affiliate 4 with the &quot;1&quot; value and 9 with the &quot;-1&quot; value). . Consider the random variable $y := sign( alpha + z)$ for $z$ as with the CDF, $ alpha$ fixed (we will discuss shortly how $ alpha$ is chosen). To see that $G_{z}(z)$ works here as a CDF, we can write $P(y= 1)$ and $P(y= -1)$ in terms of the CDF. $P(y=1)$ can be rewritten as $P(z &gt; - alpha)$ since $y=1$ when $ alpha + z &gt; 0$ or $z &gt; - alpha$. Thus, we can use the CDF to solve this, by writing $$P(y=1) = P(z &gt; - alpha) = 1-G_{z}(-a) = 1- frac{1}{1+e^{-(- alpha)}} = 1 - frac{1}{1+e^{ alpha}}= frac{e^{ alpha}}{1+e^{ alpha}}= frac{1}{1+e^{- alpha}} $$ . Similarly, for $P(y=-1)$, we evaluate $P(z &lt; -a)$ since $y&lt;0$ when $ alpha + z &lt; 0$, or $z &lt; - alpha$. Thus we can write $$P(y = -1) = P(z &lt; - alpha) = G_Z(-a)= frac{1}{1+e^{ alpha}}$$ . From this, it is plain to see what the probability mass function for y is $P(y) = frac{1}{1+e^{- alpha y}} $, and for multiple $y$&#39;s that are independent and identically distributed (as we have with the labels for MNIST data) we can write this as $P(y) = prod_{i=1}^{N} frac{1}{1+e^{- alpha_{i} y_{i}}} $ for a fixed set of $ alpha_{i}&#39;s$. . Now that we have done some groundwork for our loss function, we can discuss how $ alpha_{i}&#39;s$ are chosen. In our MNIST dataset, if we set $ alpha_{i} = w^{T}x_{i}$ for $x_{i}$ being the pixel vector for the $i^{th}$ data record, we can solve our classification problem by maximizing the resulting function. This function would look like $$H(w) = prod_{i=1}^{N} frac{1}{1+e^{-w^{T}x_{i} y_{i}}} $$. . So why does maximizing $H(w)$ solve our problem? We want to maximize $H(w)$ because this will give us the probability that the parities of $w^Tx_i$ and $y_i$ match. Indeed if the parities differ, then $w^{T}x_{i} y_{i}$ would be a negative expression, which would cancel with the negative inside the function, so $H(w)$ would simplify to $ frac{1}{1+e^{w^T x_i y_i}}$, which makes $e^{w^T x_i y_i}$ large, and the overall expression very small. Note that when the parities of $w^Tx_i$ and $y_i$ differ, the weight vector is not correctly classifying the digit. For example, perhaps we have the label of a digit be 9, which can correspond to -1. Recall we want $y_{i}= sign(w^{T} x_{i})$, so if the sign of $w^{T} x_{i}$ is 1, we know our weight vector is not correctly &quot;predicting&quot; the label. Thus our problem becomes $$ max_{w} prod_{i=1}^{N} frac{1}{1+e^{-w^{T}x_{i} y_{i}}} $$ . Now, this is equivalent to the problem $ max_{w} sum_{i=1}^{N} log biggl( frac{1}{1+e^{-w^{T}x_{i} y_{i}}} biggr)$, and log makes the calculations a bit easier to work with, so we want to apply it. Now, note that the problem $$ max_{w} sum_{i=1}^{N} log biggl( frac{1}{1+e^{-w^{T}x_{i} y_{i}}} biggr)$$ is equivalent to solving: $$ min_{w} biggl[1 - sum_{i=1}^{N} log biggl( frac{1}{1+e^{-w^{T}x_{i} y_{i}}} biggr) biggr] $$ which simplifies to $ min_{w} frac{1}{N} sum_{i=1}^{N} log(1+e^{-w^{T}x_{i} y_{i}})$. Thus, our primal optimization problem becomes $$ min_{w} frac{1}{N} sum_{i=1}^{N} log(1+e^{-w^{T}x_{i} y_{i}})$$ To solve this optimization problem, we will use gradient descent and its variations. . What is gradient descent? . Gradient descent (GD) is a popular algorithm used in data science when solving classification problems. GD uses datapoints and the loss function, derived from the data distribution to find a set of weights that, when applied to the datapoints, or data records, classify them into their label. In the MNIST dataset, this is useful because we are attepting to solve a classification problem: we are given training data of handwritten digits, and we want to find a weight vector that best classifies each data record to the label of the digit between two distinct digits. In mathematical terms, this means we want a weight vector so that the following equation holds for the correct label $l$ corresponding to each data record $x_{i}$, $$ w^{T}x_{i} = l$$ where $ l = [-1,1] $. To obtain this weight vector, we want to find a $w$ such that the loss is at its minimum, ie as its gradient approaches 0. Thus, we want to head in the direction of the negative gradient until we get close to 0, and thus approach a minimum for the loss fuction. We update the weight vector iteratively until we reach a stopping criteria, which can be determined by passing a set threshold for the norm of the gradient or the loss function, for example. Thus the update steps for gradient descent look like: $$ w^{(t+1)} = w^{(t)} - mu nabla_{w}f$$ where $f$ is the loss function and $ mu$ denotes the learning rate, which can be determined in a number of different ways, and which is further explored in the different variations of gradient descent. With no variations, we take $ mu$ to be a scalar constant, determined through trial and error, a process referred to as parameter tuning. In some the variations, the learning rate is updated iteratively, allowing it to be a function of the other variables. . The gradient of the loss function is $$ nabla_{w}f(w)= frac{1}{N} sum_{i=1}^{N} frac{- x_{i} y_{i}}{1+e^{w^{T}x_{i} y_{i}}}$$ . Gradient Descent Demo . Let&#39;s go through a demo of gradient descent. To do this, we need to convert the file to a numpy array and define some helper methods. . train = (train.to_numpy()) test = (test.to_numpy()) . def select_digits(dataset, label, num_samples): &#39;&#39;&#39; This method subsets our data for a specific digit and a set number of samples. Parameters: a dataset, a label (scalar value in [0,1]) of the digit, and a number of samples we want to subset Output: a subsampled dataset of the givel label &#39;&#39;&#39; indicator = dataset[:, 0] == label subsampled_dataset = dataset[indicator, :] subsampled_dataset = subsampled_dataset[0:num_samples, :] return subsampled_dataset . def process_data(dataset, class_labels, num_samples, method): &#39;&#39;&#39; This method is meant to essentially parse our data, sumsample it, relabel the digits into -1,1 for regression, and create a subset we can then use to perform the required calculations for gradient descent. We allow a method for a &quot;split&quot; option. The reason we define this method is so that we can split the data if required, or simply relabel it and keep it as one dataset, depending on what we need (ie if we need to preserve index) Parameters: a dataset, a set of two digits we want to compare in GD, a number of samples (which will be passed into the select_digits function), and a method (by default none) Output: Without the split method, we output a subset of size num_samples of the digits we want, which are relabelled as 1,-1 With the split method, we output a vector of length num_samples of the labels of the digits, and a subsampled dataset of pixels of our desired digits &#39;&#39;&#39; dataset0 = select_digits(dataset, class_labels[0], num_samples) dataset1 = select_digits(dataset, class_labels[1], num_samples) dataset0[:, 0] = -1 dataset1[:, 0] = 1 dataset01 = np.concatenate((dataset0, dataset1), axis=0) if method == &#39;split&#39;: y = dataset01[:, 0] X = dataset01[:, 1:] return (X, y) else: return dataset01 . def loss(w, df): &#39;&#39;&#39; This function simply calculates the loss we outlined above, log scaled. Parameters: a weight vector w, a dataset df Output: a scalar value of the loss, log scaled to make it easier to see in the plot &#39;&#39;&#39; lbl = df[:, 0] df = (df[:, 1:]) / 255 summands = np.array([(1 + np.exp(-w.T @ x * k)) for (x, k) in zip(df, lbl)]) return np.log(np.mean(summands)) . def gradient(w, df): &#39;&#39;&#39; This function calculates the gradient of the loss function from the discussion. Parameters: a weight vector w, a dataset df Output: a gradient vector (length 784) of the function &#39;&#39;&#39; N = df.shape[0] lbl = df[:, 0] df = (df[:, 1:]) / 255 summands = -1 / N * np.array( [y * x / (1 + np.exp(w.T @ x * y)) for (x, y) in zip(df, lbl)]) return np.sum(summands, axis=0) . This method is the GD algorithm itself, it performs the update steps for the weights, calculates the loss, etc. The stopping criteria we chose here is when the gradient approaches 0 and becomes less than the tol parameter, which is a sort of threshold we can set for the gradient to be considered &quot;close enough&quot; to 0. For variants of GD, we will define new functions in place of this one to see the demos, but our other helper functions will remain the same. . def GD(w_init, learning_rate, X, tol): &#39;&#39;&#39; This function performs the gradient descent update steps, with the stoppping criteria as described above. Parameters: an initial weight vector w, a learning rate, a dataset X, a tolerance threshold for the gradient norm Output: a list of the loss values at each iterate and a trained weight vector w &#39;&#39;&#39; c = 0 loss_vals = [] m = learning_rate err = tol + 1 w = w_init while (err &gt; tol) and (c &lt; 1500): p = gradient(w, X) w -= p * m h = loss(w, X) loss_vals.append(h) c += 1 err = np.linalg.norm(p) return loss_vals, w . def plotloss(loss_list, title, vis=None): &#39;&#39;&#39; This function plots the loss values at each iterate. Parameters: a list loss_list which contains loss values at each iterate, a title given as a string for the plot, and a vis parameter, which defaults to plotting individual points but can be used to plot a line with no points instead Output: a plot of the loss &#39;&#39;&#39; if vis == &#39;smooth&#39;: marker = &#39;-&#39; else: marker = &#39;-o&#39; fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(loss_list, marker) ax.set_xlabel(&quot;Iteration&quot;, fontsize=20) ax.set_ylabel(&quot;Loss&quot;, fontsize=20) ax.set_title(title, fontsize=28) . def classify(weight_vector, class_labels): &#39;&#39;&#39; This function classifies the test dataset using the trained weight vector from our model. Parameters: a trained weight vector w, a set of class labels (to be passed into the process_data method for the test set) these should be the same digits we compared in the GD algorithm Output: two scalar values in [0,1]: one describing the classification accuracy of the model for the training data, one describing the same for the test data &#39;&#39;&#39; X_test, y_test = process_data(test, class_labels=class_labels, num_samples=500, method=&#39;split&#39;) y_train = train_sample[:, 0] X_train = train_sample[:, 1:] train_labels = np.sign(weight_vector @ X_train.T) test_labels = np.sign(weight_vector @ X_test.T) train_accuracy = np.sum(train_labels == y_train) / 1000 test_accuracy = np.sum(test_labels == y_test) / 1000 return train_accuracy, test_accuracy . This next bit of code will help us run the code and tell us a little bit about the performance of it in terms of time. We will try this with digits [4,9]; [0,1]; [3,8]; . train_sample = process_data(train, class_labels=[9, 4], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) gd_loss_list, gd_weights = GD(w_init=v, learning_rate=10e-2, X=train_sample, tol=10e-6) toc = time() print(f&quot; Gradient Descent runtime: {toc-tic:.2f} seconds.&quot;) . Gradient Descent runtime: 47.58 seconds. . plotloss(gd_loss_list, title=&#39;GD Loss for digits 4,9&#39;, vis=None) . train_accuracy, test_accuracy = classify(gd_weights, class_labels=[9, 4]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.981 The accuracy rate for the test data is: 0.925 . The accuracy for 4, 9 was fairly high. Now let&#39;s try 0,1. . train_sample = process_data(train, class_labels=[0, 1], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) gd_loss_list01, gd_weights01 = GD(w_init=v, learning_rate=10e-2, X=train_sample, tol=10e-6) toc = time() print(f&quot; Gradient Descent runtime: {toc-tic:.2f} seconds.&quot;) . Gradient Descent runtime: 47.95 seconds. . plotloss(gd_loss_list01, title=&#39;GD Loss for digits 0,1&#39;) . train_accuracy, test_accuracy = classify(gd_weights01, class_labels=[0, 1]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.998 The accuracy rate for the test data is: 0.995 . The accuracy for 0,1 is higher. This makes sense because 0,1 are very dissimilar in terms of pixel overlap, whereas 4 and 9 have a bit more overlap, so we can expect less accuracy comparing those digits. Now let&#39;s compare 3 and 8, another set of fairly similar-looking digits. . train_sample = process_data(train, class_labels=[3, 8], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) gd_loss_list38, gd_weights38 = GD(w_init=v, learning_rate=10e-2, X=train_sample, tol=10e-6) toc = time() print(f&quot; Gradient Descent runtime: {toc-tic:.2f} seconds.&quot;) . Gradient Descent runtime: 46.57 seconds. . plotloss(gd_loss_list38, title=&#39;GD Loss for digits 3,8&#39;) . train_accuracy, test_accuracy = classify(gd_weights38, class_labels=[3, 8]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.972 The accuracy rate for the test data is: 0.919 . Note the accuracy is a bit lower for these digits, as expected. . Gradient descent with momentum . Gradient descent with momentum, or Polyak momentum, is an improvement upon GD that uses the same basic idea of the GD, but gets a little more precise by incentivizing the algorithm to move further in the direction of the negative gradient if the previous iterate was also moving in that direction, and take shorter steps in the wrong direction (ie where the loss increases) by penalizing those steps where the direction of the gradient and the previous step don&#39;t match. It does this by adding a &quot;momentum&quot; term, so the update steps look like: $$ w^{(t+1)} = w^{(t)} - mu nabla_{w}f + beta (w^{t} - w^{t-1})$$ Here, $ beta$ is chosen in a similar manner to the learning rate $ mu$; in the demo, we will limit our methods to trial and error for $ beta$. However, a more . Gradient descent with classical momentum demo . We will use the following function for GD with momentum; this function is similar to the GD function we had, but we will include a momentum term to see how it compares to vanilla GD. Going forward we will only compare the digit sets [0,1] and [3,8] to focus on the impact in accuracy of the methodology on similar and dissimilar digits . def GD_momentum(w_init, learning_rate, beta, X, tol): &#39;&#39;&#39; This function performs the gradient descent with classical momentum update steps, with the stoppping criteria as before. Parameters: an initial weight vector w, a learning rate, a beta value for the momentum strength, a dataset X, a tolerance threshold for the gradient norm Output: a list of the loss values at each iterate and a trained weight vector w &#39;&#39;&#39; c = 0 loss_vals = [] m = learning_rate err = tol + 1 w = w_init b = beta temp = w.copy() while (err &gt; tol) and (c &lt; 1500): p = gradient(w, X) if c &gt; 0: w_prev = temp.copy() temp = w.copy() w += -p * m + b * (w - w_prev) else: w -= p * m h = loss(w, X) loss_vals.append(h) c += 1 err = np.linalg.norm(p) return loss_vals, w . train_sample = process_data(train, class_labels=[0, 1], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) gdm_loss_list, gdm_weights = GD_momentum(w_init=v, learning_rate=10e-2, beta=0.9, X=train_sample, tol=10e-6) toc = time() print(f&quot; GD with momentum runtime: {toc-tic:.2f} seconds.&quot;) . GD with momentum runtime: 46.94 seconds. . plotloss(gdm_loss_list, title=&#39;GD with Momentum Loss for 0,1&#39;) . train_accuracy, test_accuracy = classify(gdm_weights, class_labels=[0, 1]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 1.0 The accuracy rate for the test data is: 0.998 . train_sample = process_data(train, class_labels=[4, 9], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) gdm_loss_list49, gdm_weights49 = GD_momentum(w_init=v, learning_rate=10e-2, beta=0.9, X=train_sample, tol=10e-6) toc = time() print(f&quot; GD with momentum runtime: {toc-tic:.2f} seconds.&quot;) . GD with momentum runtime: 48.06 seconds. . plotloss(gdm_loss_list49, title=&#39;GD with Momentum Loss for 4,9&#39;) . train_accuracy, test_accuracy = classify(gdm_weights49, class_labels=[4, 9]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.999 The accuracy rate for the test data is: 0.937 . Note the accuracy on both the training and the test data is higher with momentum. We will now explore a variant of GD with momentum, called Nesterov&#39;s Acceleration Gradient (also called NAG). It is similar to GD with momentum, but it performs the algorithm in two update steps: . First, we take a step in the &quot;momentum direction&quot;, the direction of the previous two steps. | Next, we take a gradient descent step from the momentum step, ie toward the direction of the negative gradient. | By doimg this, the algorithm includes a penalty term in the gradient as well if the previous two steps were not in the same direction (unlike momentum, which only included this penalty with the overall algorithm). In mathematical terms, we need to define a new variable $y^{(t)}$ and update as follows: . $$ y^{(t)} = w^{(t)} + beta (w^{(t)} - w^{(t-1)})$$ . $$ w^{(t+1)} = y^{(t)} - mu nabla f(y^{(t)})$$ We can combine these to get: $$ w^{(t+1)} = w^{(t)} + beta (w^{(t)} - w^{(t-1)}) - mu nabla f(w^{(t)} + beta (w^{(t)} - w^{(t-1)}))$$ . Nesterov&#39;s Accelerated Gradient demo . def NAG(w_init, learning_rate, beta, X, tol): &#39;&#39;&#39; This function performs the Nesterov&#39;s Accelerated Gradient update steps, with the stoppping criteria as described above. Parameters: an initial weight vector w, a learning rate, a beta value for the momentum strength, a dataset X, a tolerance threshold for the gradient norm Output: a list of the loss values at each iterate and a trained weight vector w &#39;&#39;&#39; c = 0 loss_vals = [] m = learning_rate err = tol + 1 w = w_init b = beta temp = w.copy() while (err &gt; tol) and (c &lt; 1500): if c &gt; 0: w_prev = temp.copy() temp = w.copy() y = w + b * (w - w_prev) w = y - m * gradient(y, X) else: w -= m * gradient(w, X) h = loss(w, X) loss_vals.append(h) err = np.linalg.norm(h) c += 1 return loss_vals, w . train_sample = process_data(train, class_labels=[0, 1], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) nag_loss_list, nag_weights = NAG(w_init=v, learning_rate=10e-2, beta=0.9, X=train_sample, tol=10e-6) toc = time() print(f&quot; Nesterovs Accelerated Gradient runtime: {toc-tic:.2f} seconds.&quot;) . Nesterovs Accelerated Gradient runtime: 49.51 seconds. . plotloss(nag_loss_list, title=&#39;Nesterovs Accelerated Gradient Loss for 0,1&#39;) . train_accuracy, test_accuracy = classify(nag_weights, class_labels=[0, 1]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 1.0 The accuracy rate for the test data is: 0.998 . Note the accuracy is the same as classical momentum for the digits 0,1. Let&#39;s see if we see a difference in similar-looking digits like 4 and 9. . train_sample = process_data(train, class_labels=[4, 9], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) nag_loss_list49, nag_weights49 = NAG(w_init=v, learning_rate=10e-2, beta=0.9, X=train_sample, tol=10e-6) toc = time() print(f&quot; Nesterovs Accelerated Gradient runtime: {toc-tic:.2f} seconds.&quot;) . Nesterovs Accelerated Gradient runtime: 48.67 seconds. . plotloss(nag_loss_list49, title=&#39;Nesterovs Accelerated Gradient Loss for 4,9&#39;) . train_accuracy, test_accuracy = classify(nag_weights49, class_labels=[4, 9]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.999 The accuracy rate for the test data is: 0.939 . Note the accuacy is higher with NAG than classical momentum for digits 4,9, but the improvement is very slight, which makes sense since our learning rate is small. With a larger sample size or more iterations, we might see some more distict differences in accuracy. . Conjugate Gradient Descent . Conjugate gradient descent is yet another variant of GD that uses conjugacy in order to converge more quickly and accurately. Conjugacy occurs when a set of vectors ${P_{1}, P_{2}, ..., P_{k}}$ and a symmetric positive definate matrix $A$ are such that ${P_{i}}^T A P_{j} = 0$ for all $i,j$ where $i neq j$. We can minimize the loss by successively minimizing it along individual directions in a conjugate set. The conjugate direction method does this by using a predefined conjugate set of vectors ${P_{1}, P_{2}, ..., P_{k}}$, and updating weights like so: $$ w^{(t+1)} = w^{(t)} + alpha_{t} P_{t} $$ $$ alpha_{t} = argmax_{a in R} F(w^{t} + aP_{t})$$ . Short detour into backtracking line search: . We would like to solve for $ alpha_{t} $ using something like exact line search. Exact line search, also called backtracking line search, is a method of computing the learning rate at every iterate to make sure that it really is moving in a descent direction. To do this, we need to check if Armijo conditions are met, and if they are not, we need to make the learning rate smaller by some factor. It can be shown that if the learning rate meets the Armijo criteria, we can be sure we are moving to an descent direction more accurately (instead of overstepping or moving too slowly towards a local loss minimum). We claim a learning rate meets the Armijo condition if the following holds: $$ F(w - mu nabla F(w)) leq F(w) - c_{1} mu || nabla F(w)||^{2}$$ Note we fix $c_{1} in [0,1/2)$ in this definition. This condition, along with the following condition, make up a set of conditions called Wolfe&#39;s conditions: $$| nabla F(w^{(t)} + alpha_{t}P_{t})^{T}P_{t}| leq -c_{2} nabla F_{t}^{T}P_{t}$$ Here we pick $c_{2} in [c_{1}, frac{1}{2}]$. We will use Wolfe&#39;s conditions for the demo of CGD. . Back to conjugate methods: This summarizes the conjugate direction method. However, we can improve upon this by using conjugate gradient method in order to avoid storing an extra set of conjugate vectors (${P_{1}, P_{2}, ..., P_{k}}$) by computing a vector at each iteration that is conjugate to all the previous vectors. We can do this by setting $$P_{t} = - nabla F(w^{(t)}) + beta_{t} P_{t-1}$$ We also need to iterate $ beta_{t}$ in order to enforce conjugacy. It can be shown that if we pick $$ beta_{t} = frac{P_{t-1}^{T} A nabla F(w^{(t)})}{P_{t-1}^{T} P_{t-1}} $$ where $A$ is the pixel matrix, we can enforce conjugacy. Other methods of updating $ beta$ that preserve conjugacy include the Fletcher-Reeves algorithm, which updates $ beta$ using the following equation: $$ beta_{t+1}^{F-R}= frac{ nabla F_{t+1}^{T} nabla F_{t+1}}{ nabla F_{t}^{T} nabla F_{t}}$$ We can also use the Polak-Ribire update method, which updates $ beta$ via $$ beta_{t+1}^{P-R} = frac{ nabla F_{t+1}^{T}( nabla F_{t+1}^{T} - nabla F_{t}^{T})}{|| nabla F_{t})||^2}$$ . In the demo below, we will explore the Fletcer-Reeves and Polak-Ribire variants of conjugate gradient descent for our dataset. Our update steps will be: $$ w^{(t+1)} = w^{(t)} + alpha_{t} P_{t} $$ . $ alpha_{t}$ - update via backtracking line search $$ beta_{t+1}^{F-R}= frac{ nabla F_{t+1}^{T} nabla F_{t+1}}{ nabla F_{t}^{T} nabla F_{t}} $$ or $$ beta_{t+1}^{P-R} = frac{ nabla F_{t+1}^{T}( nabla F_{t+1}^{T} - nabla F_{t}^{T})}{|| nabla F_{t})||^2}$$ $$ P_{t+1} = nabla F_{t+1} + beta_{t+1} P_{t} $$ Note we will need to define a new method for both backtacking line search and a check for the Wolfe criterion. . Conjugate Gradient Descent Demo . def isWolfe(m, p, w, X): &#39;&#39;&#39; This method checks if the learning rate meets the Wolfe conditions. Parameters: a learning rate m, a conjugate vector p, the weight vector w, and the pixel data X Output: A True or False value of whether the &#39;&#39;&#39; c1 = 0.4 c2 = 0.5 if [ loss((w + m * p), X) &lt;= loss(w, X) + c1 * m * np.dot(gradient(w, X).T, p) ]: if [ abs(np.dot(gradient(w + m * p, X), (X[:, 1:]).T / 255)) &lt;= -c2 * np.dot(gradient(w, X), p) ]: return True else: return False . def lineSearch(m, p, w, X): &#39;&#39;&#39; This method computes the learning rate via the backtracking line search, as outlined in the discussion. Parameters: a learning rate m, a conjugate vector p, a weight vector w, and the pixel data X Output: a scalar learning rate m &#39;&#39;&#39; while not isWolfe(m, p, w, X): m = 0.8 * m else: return m . def CGD(w_init, learning_rate, X, tol, method): &#39;&#39;&#39; This function performs the SGD update steps, with the stoppping criteria as before Parameters: an initial weight vector w, a learning rate, a dataset X, a tolerance threshold for the gradient norm Output: a list of the loss values at each iterate and a trained weight vector w &#39;&#39;&#39; if method not in [&#39;F-R&#39;, &#39;P-R&#39;]: raise ValueError(&#39;Invalid method for CGD&#39;) c = 0 loss_vals = [] m = learning_rate err = tol + 1 w = w_init new_grad = gradient(w, X) p = -new_grad while (c &lt; 1500) and (err &gt; tol): current_grad = new_grad.copy() m = lineSearch(m, p, w, X) w += m * p new_grad = gradient(w, X) if method == &#39;F-R&#39;: beta = np.dot(new_grad, new_grad) / np.dot(current_grad, current_grad) else: if method == &#39;P-R&#39;: beta = np.dot(new_grad, new_grad - current_grad) / np.dot( current_grad, current_grad) p = -new_grad + beta * p h = loss(w, X) loss_vals.append(h) err = np.linalg.norm(h) c += 1 return loss_vals, w . train_sample = process_data(train, class_labels=[0, 1], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) cgd_loss_listFR, cgd_weightsFR = CGD(w_init=v, learning_rate=10e-2, X=train_sample, tol=10e-6, method=&#39;F-R&#39;) toc = time() print( f&quot; Conjugate gradient descent with Fletcher-Reeves runtime: {toc-tic:.2f} seconds.&quot; ) . Conjugate gradient descent with Fletcher-Reeves runtime: 186.05 seconds. . plotloss(cgd_loss_listFR, title=&#39;Conjugate Gradient Descent with Fletcher-Reeves Loss for 1,0&#39;) . train_accuracy, test_accuracy = classify(cgd_weightsFR, class_labels=[0, 1]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 1.0 The accuracy rate for the test data is: 0.997 . Clearly, this processs took a lot longer than other methods; this is expected due to the backtracking line search. We can also see we even lost a bit of accuracy with this method. Let&#39;s see if the same holds for 4,9. . train_sample = process_data(train, class_labels=[4, 9], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) cgd_loss_list49FR, cgd_weights49FR = CGD(w_init=v, learning_rate=10e-2, X=train_sample, tol=10e-6, method=&#39;F-R&#39;) toc = time() print( f&quot; Conjugate gradient descent with Fletcher-Reeves runtime: {toc-tic:.2f} seconds.&quot; ) . Conjugate gradient descent with Fletcher-Reeves runtime: 185.34 seconds. . plotloss(cgd_loss_list49FR, title=&#39;Conjugate Gradient Descent Loss with Fletcher-Reeves for 4,9&#39;) . train_accuracy, test_accuracy = classify(cgd_weights49FR, class_labels=[4, 9]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 1.0 The accuracy rate for the test data is: 0.931 . train_sample = process_data(train, class_labels=[0, 1], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) cgd_loss_listPR, cgd_weightsPR = CGD(w_init=v, learning_rate=10e-2, X=train_sample, tol=10e-6, method=&#39;P-R&#39;) toc = time() print( f&quot; Conjugate gradient descent with Polak-Ribire runtime: {toc-tic:.2f} seconds.&quot; ) . Conjugate gradient descent with Polak-Ribire runtime: 219.72 seconds. . plotloss(cgd_loss_listPR, title=&#39;Conjugate Gradient Descent Loss with Polak-Ribire for 1,0 &#39;) . train_accuracy, test_accuracy = classify(cgd_weightsPR, class_labels=[0, 1]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.998 The accuracy rate for the test data is: 0.995 . train_sample = process_data(train, class_labels=[4, 9], num_samples=500, method=None) tic = time() v = np.ones(train_sample.shape[1] - 1) cgd_loss_list49PR, cgd_weights49PR = CGD(w_init=v, learning_rate=10e-2, X=train_sample, tol=10e-6, method=&#39;P-R&#39;) toc = time() print( f&quot; Conjugate gradient descent with Polak-Ribire runtime: {toc-tic:.2f} seconds.&quot; ) . Conjugate gradient descent with Polak-Ribire runtime: 197.08 seconds. . plotloss(cgd_loss_list49PR, title=&#39;Conjugate Gradient Descent Loss with Polak-Ribire for 4,9&#39;) . train_accuracy, test_accuracy = classify(cgd_weights49PR, class_labels=[4, 9]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.981 The accuracy rate for the test data is: 0.937 . The accuracy has improved here in comparison to some previous methods. However, it is clear this method takes a long time to converge. We can modify that with stochastic gradient descent. . Stochastic Gradient Descent . We&#39;ve looked at various GD variants and gotten to a good accuracy using those methods. However, these have been pretty computationally expensive, so we should examine methods that are faster or converge sooner. One method that is computationally less expensive than vanilla GD is stochastic gradient descent, or SGD. GD and SGD share the same idea in that they both optimize using the direction of the negative gradient; in SGD however, we seek to make the gradient calculation (which happens at each iteration) faster, so instead of taking the gradient of all of the datapoints, we take the gradient of a single datapoint, sampled at random without replacement at each iterate. Our update step then looks like: $$ w^{(t+1)} = w^{(t)} - mu nabla_{w_{t}}f$$ where $ nabla_{w_{i_{t}}}f $ denotes the gradient of the loss function at iterate t with respect to some datapoint $x_{i_{t}}$ (here, $i_{t}$ is a random variable). This will allow for faster calculations of the gradient, since we only have to evaluate at one point; however, since the datapoints are chosen at random, we can expect to see more variance in the loss, as the datapoints might give different directions for the gradient. In the demo, we will redefine the function that performs the update step for the weights as well as the gradient calculation method. We&#39;ll also need to use a smaller learning rate in order to reduce the variance. . Stochastic Gradient Descent demo . def stochastic_gradient(w, x_i): &#39;&#39;&#39; This method computes the gradient for SGD with respect to one randomly chosen datapoint. Parameters: a weight vector w, a randomly chosen datapoint x_i Output: a length 784 vector, the gradient of the loss at datapoint x_i &#39;&#39;&#39; y = x_i[0, 0] x_i = ((x_i[:, 1:]).T * y) / 225 return (-x_i) / (1 + np.exp(np.dot(w.T, x_i))) . def SGD(w_init, learning_rate, X, tol): &#39;&#39;&#39; This function performs the SGD update steps, with the stoppping criteria as before Parameters: an initial weight vector w, a learning rate, a dataset X, a tolerance threshold for the gradient norm Output: a list of the loss values at each iterate and a trained weight vector w &#39;&#39;&#39; c = 0 loss_vals = [] m = learning_rate err = tol + 1 w = w_init N = X.shape[0] while (err &gt; tol) and (c &lt; 1500): random_index = np.random.choice(N, size=1, replace=True) dp_rand = X[random_index, :] p = stochastic_gradient(w, dp_rand) w -= m * p h = loss(w, X) loss_vals.append(h) err = np.linalg.norm(h) c += 1 return loss_vals, w . train_sample = process_data(train, class_labels=[1, 0], num_samples=500, method=None) tic = time() v = np.zeros((train_sample.shape[1] - 1, 1)) sgd_loss_list, sgd_weights = SGD(w_init=v, learning_rate=10e-3, X=train_sample, tol=10e-6) toc = time() print(f&quot; Stochastic Gradient Descent runtime: {toc-tic:.2f} seconds.&quot;) . Stochastic Gradient Descent runtime: 22.84 seconds. . plotloss(sgd_loss_list, title=&#39;Stochastic Gradient Loss for 1,0&#39;, vis=&#39;smooth&#39;) . train_accuracy, test_accuracy = classify(sgd_weights.T, class_labels=[1, 0]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.999 The accuracy rate for the test data is: 1.0 . train_sample = process_data(train, class_labels=[4, 9], num_samples=500, method=None) tic = time() v = np.zeros((train_sample.shape[1] - 1, 1)) sgd_loss_list49, sgd_weights49 = SGD(w_init=v, learning_rate=10e-3, X=train_sample, tol=10e-6) toc = time() print(f&quot; Stochastic Gradient Descent runtime: {toc-tic:.2f} seconds.&quot;) . Stochastic Gradient Descent runtime: 21.03 seconds. . plotloss(sgd_loss_list49, title=&#39;Stochastic Gradient Loss for 4,9&#39;, vis=&#39;smooth&#39;) . train_accuracy, test_accuracy = classify(sgd_weights49.T, class_labels=[4, 9]) print(&#39;The accuracy rate for the training data is: &#39;, train_accuracy) print(&#39;The accuracy rate for the test data is: &#39;, test_accuracy) . The accuracy rate for the training data is: 0.962 The accuracy rate for the test data is: 0.92 . Visualizing each algorithm&#39;s performance . fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(gd_loss_list01, &#39;green&#39;, label = &#39;Gradient Descent&#39;) ax.plot(gdm_loss_list, &#39;red&#39;, label = &#39;Momentum&#39;) ax.plot(nag_loss_list, &#39;red&#39;, label = &#39;NAG&#39;) ax.plot(cgd_loss_listFR, &#39;blue&#39;,label = &#39;CGD-FR&#39;) ax.plot(cgd_loss_listPR, &#39;cyan&#39;, label = &#39;CGD-PR&#39;) ax.plot(sgd_loss_list, &#39;yellow&#39;, label =&#39;SGD&#39;) ax.legend() ax.set_xlabel(&quot;Iteration&quot;, fontsize=20) ax.set_ylabel(&quot;Loss&quot;, fontsize=20) ax.set_title(&#39;GD algorithms for 0,1&#39;, fontsize=28) . Text(0.5, 1.0, &#39;GD algorithms for 0,1&#39;) . fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(gd_loss_list, &#39;green&#39;, label = &#39;Gradient Descent&#39;) ax.plot(gdm_loss_list49, &#39;red&#39;, label = &#39;Momentum&#39;) ax.plot(nag_loss_list49, &#39;red&#39;, label = &#39;NAG&#39;) ax.plot(cgd_loss_list49FR, &#39;blue&#39;,label = &#39;CGD-FR&#39;) ax.plot(cgd_loss_list49PR, &#39;cyan&#39;, label = &#39;CGD-PR&#39;) ax.plot(sgd_loss_list49, &#39;yellow&#39;, label =&#39;SGD&#39;) ax.legend() ax.set_xlabel(&quot;Iteration&quot;, fontsize=20) ax.set_ylabel(&quot;Loss&quot;, fontsize=20) ax.set_title(&#39;GD algorithms for 4,9&#39;, fontsize=28) . Text(0.5, 1.0, &#39;GD algorithms for 4,9&#39;) .",
            "url": "https://vrindachaa.github.io/wss/machine%20learning/python/jupyter/random%20forest/decision%20tree/xgboost/lgbm/extratrees/knn/scikit-learn/pandas/data%20science/2021/01/01/MNISTClassificationwithGradientDescent.html",
            "relUrl": "/machine%20learning/python/jupyter/random%20forest/decision%20tree/xgboost/lgbm/extratrees/knn/scikit-learn/pandas/data%20science/2021/01/01/MNISTClassificationwithGradientDescent.html",
            "date": "  Jan 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a level 1 heading in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Heres a footnote 1. Heres a horizontal rule: . . Lists . Heres a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes and . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.&#8617; . |",
            "url": "https://vrindachaa.github.io/wss/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": "  Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! My name is Vrinda Chauhan and Im a recent graduate from UCSD, where I studied mathematics, statistics, and data science. I have a deep interest in machine learning and deep learning, specifically in context of signal processing. My projects display some of my academic, professional, and personal projects, and Im interested in applying my knowledge to real world data to make predictions and develop my work as a data scientist. Im open to collaboration on Kaggle competitions, projects, and hackathons, and Id be happy to discuss my work in more detail. Please feel free to reach out to me or connect with me on LinkedIn. fastpages 1. . a portfolio &amp; blog about some of my programming and data science projects.&#8617; . |",
          "url": "https://vrindachaa.github.io/wss/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ sitemap.xml | absolute_url }} | .",
          "url": "https://vrindachaa.github.io/wss/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}